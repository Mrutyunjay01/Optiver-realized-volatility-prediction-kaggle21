{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "class cfg:\n",
    "    \n",
    "    paths = {\n",
    "        # train path\n",
    "        \"train_csv\": \"./../inp/Raw Data/train.csv\",\n",
    "        \"train_book\": \"./../inp/Raw Data/book_train.parquet\",\n",
    "        \"train_trade\": \"./../inp/Raw Data/trade_train.parquet\",\n",
    "\n",
    "        # test path\n",
    "        \"test_csv\": \"./../inp/Raw Data/test.csv\",\n",
    "        \"test_book\": \"./../inp/Raw Data/book_test.parquet\",\n",
    "        \"test_trade\": \"./../inp/Raw Data/trade_test.parquet\",\n",
    "\n",
    "        # model paths\n",
    "        \"xgb_baseline\": \"./../models/xgbBaseline/\",\n",
    "        \"lgb_baseline\": \"./../models/lgbBaseline/\"\n",
    "    }\n",
    "\n",
    "    feature_dict_book = {\n",
    "        \"wap1\": [np.sum, np.mean, np.std],\n",
    "        \"wap2\": [np.sum, np.mean, np.std],\n",
    "        \"log_return1\": [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"log_return2\": [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"wap_balance\": [np.sum, np.mean, np.std],\n",
    "        \"volume_imbalance\": [np.sum, np.mean, np.std],\n",
    "        \"total_volume\": [np.sum, np.mean, np.std],\n",
    "        \"price_spread1\": [np.sum, np.mean, np.std],\n",
    "        \"price_spread2\": [np.sum, np.mean, np.std],\n",
    "        \"bid_spread\": [np.sum, np.mean, np.std],\n",
    "        \"ask_spread\": [np.sum, np.mean, np.std],\n",
    "    }\n",
    "\n",
    "    feature_dict_trade = {\n",
    "        \"log_return\": [calculate_rv],\n",
    "        \"seconds_in_bucket\": [count_unique],\n",
    "        \"size\": [np.sum],\n",
    "        \"order_count\": [np.mean]\n",
    "    }\n",
    "\n",
    "    model_params = {\n",
    "        \"xgb_bl\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"nthread\": -1,\n",
    "            \"eta\": 0.3,\n",
    "            \"max_depth\": 8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"sampling_method\": \"uniform\",\n",
    "            # \"tree_method\": \"gpu_hist\"\n",
    "        },\n",
    "        \"lgb_bl\": {\n",
    "            \"objective\": \"rmse\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"learning_rate\": 0.05,\n",
    "        }\n",
    "    }\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature utils\n",
    "def calculate_wap(df, rank=\"1\"):\n",
    "    \"\"\"\n",
    "    Weighted Average Pricing for a stock at a given time ID is given by:\n",
    "    (bid_price1 * ask_size1 + bid_size1 * ask_price1)/(bid_size1 + ask_size1)\n",
    "\n",
    "    It can further be extended to:\n",
    "\n",
    "        sum(bid_price_i * ask_size_i + bid_size_i * ask_price_i)/sum(bid_size_i + ask_size_i)\n",
    "\n",
    "    :param rank: which wap to calculate\n",
    "    :param df: parquet table containing order book\n",
    "    :return: wap for given rank\n",
    "    \"\"\"\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"bid_size{rank}\"] * df[f\"ask_price{rank}\"]) / (df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "\n",
    "\n",
    "def calculate_inter_wap(df, rank=\"1\"):\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n",
    "                df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def calculate_rv(series):\n",
    "    return np.sqrt(np.sum(np.square(series)))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def get_stats_window(df, seconds_in_bucket, features_dict, add_suffix=False):\n",
    "    df_feature = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(features_dict).reset_index()\n",
    "    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n",
    "\n",
    "    if add_suffix:\n",
    "        df_feature = df_feature.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "\n",
    "    return df_feature\n",
    "    pass\n",
    "\n",
    "\n",
    "def window_stats(df, feature_dict, second_windows):\n",
    "    df_merged = get_stats_window(df, seconds_in_bucket=0, features_dict=feature_dict)\n",
    "\n",
    "    temp_dfs = []\n",
    "    for window in second_windows:\n",
    "        temp_dfs.append((window, get_stats_window(df, seconds_in_bucket=window, features_dict=feature_dict, add_suffix=True)))\n",
    "\n",
    "    for window, temp_df in temp_dfs:\n",
    "        df_merged = df_merged.merge(temp_df, how=\"left\", left_on=\"time_id_\", right_on=f\"time_id__{window}\")\n",
    "        df_merged.drop(columns=[f\"time_id__{window}\"], inplace=True)\n",
    "\n",
    "    return df_merged\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order book features\n",
    "def get_book_features(file_path):\n",
    "    book_df = pd.read_parquet(file_path)\n",
    "\n",
    "    # calculate wap\n",
    "    book_df['wap1'] = calculate_wap(book_df, rank=\"1\")\n",
    "    book_df['wap2'] = calculate_wap(book_df, rank=\"2\")\n",
    "    book_df['iwap1'] = calculate_inter_wap(book_df, rank=\"1\")\n",
    "    book_df['iwap2'] = calculate_inter_wap(book_df, rank=\"2\")\n",
    "\n",
    "    # calculate log return\n",
    "    book_df[\"log_return1\"] = book_df.groupby([\"time_id\"])[\"wap1\"].apply(calculate_log_return)\n",
    "    book_df[\"log_return2\"] = book_df.groupby([\"time_id\"])[\"wap2\"].apply(calculate_log_return)\n",
    "    book_df[\"inter_log_return1\"] = book_df.groupby([\"time_id\"])[\"iwap1\"].apply(calculate_log_return)\n",
    "    book_df[\"inter_log_return2\"] = book_df.groupby([\"time_id\"])[\"iwap2\"].apply(calculate_log_return)\n",
    "\n",
    "    # calculate balance\n",
    "    book_df[\"wap_balance\"] = abs(book_df[\"wap1\"] - book_df[\"wap2\"])\n",
    "    book_df[\"volume_imbalance\"] = abs(\n",
    "        (book_df[\"ask_size1\"] + book_df[\"ask_size2\"]) - (book_df[\"bid_size1\"] + book_df[\"bid_size2\"]))\n",
    "    book_df[\"total_volume\"] = book_df[\"ask_size1\"] + book_df[\"ask_size2\"] + book_df[\"bid_size1\"] + book_df[\n",
    "        \"bid_size2\"]\n",
    "\n",
    "    # calculate spread\n",
    "    book_df[\"price_spread1\"] = (book_df[\"ask_price1\"] - book_df[\"bid_price1\"]) / (\n",
    "            (book_df[\"ask_price1\"] + book_df[\"bid_price1\"]) / 2)\n",
    "    book_df[\"price_spread2\"] = (book_df[\"ask_price2\"] - book_df[\"bid_price2\"]) / (\n",
    "            (book_df[\"ask_price2\"] + book_df[\"bid_price2\"]) / 2)\n",
    "\n",
    "    book_df[\"bid_spread\"] = book_df[\"bid_price1\"] - book_df[\"bid_price2\"]\n",
    "    book_df[\"ask_spread\"] = book_df[\"ask_price1\"] - book_df[\"ask_price2\"]\n",
    "\n",
    "    book_df_merged = window_stats(book_df, cfg.feature_dict_book, [450, 300, 150])\n",
    "\n",
    "    book_df_merged[\"row_id\"] = book_df_merged[\"time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    book_df_merged.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return book_df_merged\n",
    "                                                                \n",
    "# trade features\n",
    "def get_trade_features(file_path):\n",
    "    trade_df = pd.read_parquet(file_path)\n",
    "\n",
    "    trade_df[\"log_return\"] = trade_df.groupby([\"time_id\"])[\"price\"].apply(calculate_log_return)\n",
    "\n",
    "    trade_df_merged = window_stats(trade_df, cfg.feature_dict_trade, [450, 300, 150])\n",
    "\n",
    "    trade_df_merged = trade_df_merged.add_prefix(\"trade_\")\n",
    "\n",
    "    trade_df_merged[\"row_id\"] = trade_df_merged[\"trade_time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    trade_df_merged.drop([\"trade_time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return trade_df_merged                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class GetData:\n",
    "    def __init__(self, df, book_path, trade_path):\n",
    "        self.df = df.copy(deep=True)\n",
    "        self.order_book_path = book_path\n",
    "        self.trade_path = trade_path\n",
    "\n",
    "        self._get_rowid()\n",
    "\n",
    "    def _get_rowid(self):\n",
    "        self.df[\"row_id\"] = self.df[\"stock_id\"].astype(str) + \"-\" + self.df[\"time_id\"].astype(str)\n",
    "\n",
    "    def get_time_stock(self):\n",
    "        vol_cols = ['log_return1_calculate_rv', 'log_return2_calculate_rv',\n",
    "                    'log_return1_calculate_rv_450', 'log_return2_calculate_rv_450',\n",
    "                    'log_return1_calculate_rv_300', 'log_return2_calculate_rv_300',\n",
    "                    'log_return1_calculate_rv_150', 'log_return2_calculate_rv_150',\n",
    "                    'trade_log_return_calculate_rv', 'trade_log_return_calculate_rv_450',\n",
    "                    'trade_log_return_calculate_rv_300', 'trade_log_return_calculate_rv_150']\n",
    "\n",
    "        df_stock_id = self.df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "        df_time_id = self.df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "        df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "        # Merge with original dataframe\n",
    "        self.df = self.df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "        self.df = self.df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "        self.df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def process_features(self, list_stock_ids):\n",
    "        def parallel_helper(stock_id):\n",
    "            book_sample_path = os.path.join(self.order_book_path, f\"stock_id={stock_id}\")\n",
    "            trade_sample_path = os.path.join(self.trade_path, f\"stock_id={stock_id}\")\n",
    "\n",
    "            return pd.merge(get_book_features(book_sample_path), get_trade_features(trade_sample_path),\n",
    "                            on=\"row_id\",\n",
    "                            how=\"left\")\n",
    "\n",
    "        df = Parallel(n_jobs=4, verbose=1)(delayed(parallel_helper)(stock_id) for stock_id in list_stock_ids)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_features(self):\n",
    "        features_df = self.process_features(self.df[\"stock_id\"].unique())\n",
    "        self.df = self.df.merge(features_df, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "        return self.get_time_stock()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric utils\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, model, is_xgb=True):\n",
    "    y_true = model.get_label()\n",
    "\n",
    "    if is_xgb:\n",
    "        return \"RMSPE\", rmspe(y_true, y_pred)\n",
    "\n",
    "    return \"RMSPE\", rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model utils\n",
    "def feval_wrapper(y_pred, model):\n",
    "    return feval_rmspe(y_pred, model, is_xgb=False)\n",
    "\n",
    "class TrainFer:\n",
    "    def __init__(self, params_dict, n_splits, model_path, random_state=2021):\n",
    "        self.params = params_dict\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.model_path = model_path\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        oof_predictions = np.zeros(X.shape[0])\n",
    "        kfold = KFold(n_splits=self.n_splits, random_state=self.random_state, shuffle=True)\n",
    "        oof_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "            print(f\"\\nFold - {fold}\\n\")\n",
    "\n",
    "            x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "            dtrain = lgb.Dataset(x_train, y_train, weight=1/np.square(y_train), categorical_feature=[\"stock_id\"])\n",
    "            dval = lgb.Dataset(x_val, y_val, weight=1/np.square(y_val), categorical_feature=[\"stock_id\"])\n",
    "\n",
    "            model = lgb.train(params=self.params,\n",
    "                              num_boost_round=10000,\n",
    "                              train_set=dtrain,\n",
    "                              valid_sets=dval,\n",
    "                              verbose_eval=250,\n",
    "                              early_stopping_rounds=200,\n",
    "                              feval=feval_wrapper)\n",
    "\n",
    "            pickle.dump(model, open(os.path.join(self.model_path, f\"lgb_bl_{fold}.pkl\"), \"wb\"))\n",
    "            fold_preds = model.predict(x_val)\n",
    "            oof_score = rmspe(y_val, fold_preds)\n",
    "            print(f\"\\nRMSPE of fold {fold}: {oof_score}\")\n",
    "            \n",
    "            oof_scores.append(oof_score)\n",
    "            oof_predictions[val_idx] = fold_preds\n",
    "        \n",
    "        print(f\"\\nOOF Scores: {oof_scores}\\n\")\n",
    "        rmspe_score = rmspe(y, oof_predictions)\n",
    "        print(f\"OOF RMSPE: {rmspe_score}\")\n",
    "        \n",
    "    def infer(self, x_test):\n",
    "        test_predictions = np.zeros(x_test.shape[0])\n",
    "\n",
    "        for mpth in os.listdir(self.model_path):\n",
    "            model = pickle.load(open(os.path.join(self.model_path, mpth), \"rb\"))\n",
    "            test_predictions += model.predict(x_test)/5\n",
    "\n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stock_id  time_id row_id    target\n",
      "0         0        4    0-4  0.001727\n",
      "1         0       32   0-32  0.001797\n",
      "2         0       34   0-34  0.001797\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    _ = gc.collect()\n",
    "    is_train = False\n",
    "\n",
    "    model = TrainFer(cfg.model_params[\"lgb_bl\"], n_splits=5, model_path=cfg.paths[\"lgb_baseline\"])\n",
    "    \n",
    "    if is_train:\n",
    "        train = pd.read_csv(cfg.paths[\"train_csv\"])\n",
    "        train_data = GetData(train, cfg.paths[\"train_book\"], cfg.paths[\"train_trade\"])\n",
    "        train_df = train_data.get_features()\n",
    "        \n",
    "        model.train(train_df.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_df[\"target\"])\n",
    "    else:\n",
    "        test = pd.read_csv(cfg.paths[\"test_csv\"])\n",
    "        test_data = GetData(test, cfg.paths[\"test_book\"], cfg.paths[\"test_trade\"])\n",
    "        test_df = test_data.get_features()\n",
    "        \n",
    "        preds = model.infer(test_df.drop(columns=[\"row_id\", \"time_id\"])) \n",
    "        test[\"target\"] = preds\n",
    "        print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
