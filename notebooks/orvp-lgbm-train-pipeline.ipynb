{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723e6fef",
   "metadata": {
    "papermill": {
     "duration": 0.009646,
     "end_time": "2021-09-27T17:18:47.114311",
     "exception": false,
     "start_time": "2021-09-27T17:18:47.104665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cdbf78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:47.145382Z",
     "iopub.status.busy": "2021-09-27T17:18:47.139882Z",
     "iopub.status.idle": "2021-09-27T17:18:49.368692Z",
     "shell.execute_reply": "2021-09-27T17:18:49.367685Z",
     "shell.execute_reply.started": "2021-09-27T16:56:17.950864Z"
    },
    "papermill": {
     "duration": 2.245292,
     "end_time": "2021-09-27T17:18:49.368855",
     "exception": false,
     "start_time": "2021-09-27T17:18:47.123563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 2021\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4487564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:49.420660Z",
     "iopub.status.busy": "2021-09-27T17:18:49.418614Z",
     "iopub.status.idle": "2021-09-27T17:18:49.421312Z",
     "shell.execute_reply": "2021-09-27T17:18:49.421741Z",
     "shell.execute_reply.started": "2021-09-27T16:56:20.498123Z"
    },
    "papermill": {
     "duration": 0.044135,
     "end_time": "2021-09-27T17:18:49.421900",
     "exception": false,
     "start_time": "2021-09-27T17:18:49.377765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature utils\n",
    "def calculate_wap(df, rank=\"1\"):\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"bid_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n",
    "                df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "\n",
    "\n",
    "def calculate_agg_wap(df):\n",
    "    wap1 = df[\"bid_price1\"] * df[\"ask_size1\"] + df[\"bid_size1\"] * df[\"ask_price1\"]\n",
    "    wap2 = df[\"bid_price2\"] * df[\"ask_size2\"] + df[\"bid_size2\"] * df[\"ask_price2\"]\n",
    "    den = df[\"ask_size1\"] + df[\"ask_size2\"] + df[\"bid_size1\"] + df[\"bid_size2\"]\n",
    "    \n",
    "    return (wap1 + wap2)/den\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_inter_wap(df, rank=\"1\"):\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n",
    "            df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "    pass\n",
    "\n",
    "def calculate_agg_inter_wap(df):\n",
    "    iwap1 = df[\"bid_price1\"] * df[\"bid_size1\"] + df[\"ask_size1\"] * df[\"ask_price1\"]\n",
    "    iwap2 = df[\"bid_price2\"] * df[\"bid_size2\"] + df[\"ask_size2\"] * df[\"ask_price2\"]\n",
    "    den = df[\"ask_size1\"] + df[\"ask_size2\"] + df[\"bid_size1\"] + df[\"bid_size2\"]\n",
    "    \n",
    "    return (iwap1 + iwap2)/den\n",
    "    pass\n",
    "\n",
    "\n",
    "def calc_depth(df):\n",
    "    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n",
    "               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
    "    return depth\n",
    "\n",
    "\n",
    "def calc_slope(df):\n",
    "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
    "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
    "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
    "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
    "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
    "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
    "    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n",
    "\n",
    "\n",
    "def calc_dispersion(df):\n",
    "    bspread = df['bid_price1'] - df['bid_price2']\n",
    "    aspread = df['ask_price2'] - df['ask_price1']\n",
    "    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n",
    "    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n",
    "    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n",
    "    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n",
    "    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n",
    "    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n",
    "    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n",
    "\n",
    "def calc_price_impact(df):\n",
    "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
    "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
    "    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n",
    "\n",
    "\n",
    "def calc_ofi(df):\n",
    "    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n",
    "    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n",
    "    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n",
    "    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n",
    "    return a - b - c + d\n",
    "\n",
    "\n",
    "def calc_tt1(df):\n",
    "    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n",
    "    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n",
    "    return p2 - p1 \n",
    "\n",
    "\n",
    "def calculate_log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def calculate_rv(series):\n",
    "    return np.sqrt(np.sum(np.square(series)))\n",
    "\n",
    "    \n",
    "# Calculate integrated quarticity\n",
    "def calculate_rv_quarticity(series):\n",
    "    return (series.count()/3)*np.sum(series**4)\n",
    "\n",
    "# Calculate weighted volatility\n",
    "def calculate_rv_vol_weighted(series):\n",
    "    return np.sqrt(np.sum(series**2)/series.count())\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def get_stats_window(df, seconds_in_bucket, features_dict, add_suffix=False):\n",
    "    df_feature = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(features_dict).reset_index()\n",
    "    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n",
    "\n",
    "    if add_suffix:\n",
    "        df_feature = df_feature.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "\n",
    "    return df_feature\n",
    "    pass\n",
    "\n",
    "\n",
    "def window_stats(df, feature_dict, feature_dict_time, second_windows, additional_dfs=None):\n",
    "    df_merged = get_stats_window(df, seconds_in_bucket=0, features_dict=feature_dict)\n",
    "\n",
    "    if additional_dfs is not None:\n",
    "        df_merged = df_merged.merge(additional_dfs, how='left', left_on='time_id_', right_on='time_id')\n",
    "\n",
    "    temp_dfs = []\n",
    "    for window in second_windows:\n",
    "        temp_dfs.append(\n",
    "            (window,\n",
    "             get_stats_window(df, seconds_in_bucket=window, features_dict=feature_dict_time, add_suffix=True)\n",
    "             )\n",
    "        )\n",
    "\n",
    "    for window, temp_df in temp_dfs:\n",
    "        df_merged = df_merged.merge(temp_df, how=\"left\", left_on=\"time_id_\", right_on=f\"time_id__{window}\")\n",
    "        df_merged.drop(columns=[f\"time_id__{window}\"], inplace=True)\n",
    "\n",
    "    return df_merged\n",
    "    pass\n",
    "\n",
    "\n",
    "def tendency(price, vol):\n",
    "    diff = np.diff(price)\n",
    "    val = (diff / price[1:]) * 100\n",
    "    power = np.sum(val * vol[1:])\n",
    "    return power\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_stock_clusters(df, n_clusters=6):\n",
    "    pivoted_data = df.pivot(index=\"time_id\", columns=[\"stock_id\"], values=\"target\")\n",
    "    corr_pivoted = pivoted_data.corr()\n",
    "\n",
    "    clusters = KMeans(n_clusters, random_state=cfg.random_state).fit(corr_pivoted.values)\n",
    "\n",
    "    groups = []\n",
    "    for i in range(n_clusters):\n",
    "        groups.append([x-1] for x in (corr_pivoted.index+1)*(clusters.labels_ == i) if x > 0)\n",
    "    return groups\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_cluster_aggregations(df, groups):\n",
    "    feats = []\n",
    "\n",
    "    for i, idx in enumerate(groups):\n",
    "        chunk_df = df.loc[df['stock_id'].isin(idx)]\n",
    "        chunk_df = chunk_df.groupby(['time_id']).agg(np.nanmean)\n",
    "        chunk_df.loc[:, 'stock_id'] = str(i) + 'c1'\n",
    "        feats.append(chunk_df)\n",
    "\n",
    "    feats = pd.concat(feats).reset_index()\n",
    "    if \"target\" in feats.columns:\n",
    "        feats.drop(columns=['target'], inplace=True)\n",
    "\n",
    "    feats = feats.pivot(index='time_id', columns='stock_id')\n",
    "    feats.columns = [\"_\".join(x) for x in feats.columns.ravel()]\n",
    "    feats.reset_index(inplace=True)\n",
    "\n",
    "    return pd.merge(df, feats, how=\"left\", on=\"time_id\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f1132b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:49.456700Z",
     "iopub.status.busy": "2021-09-27T17:18:49.455948Z",
     "iopub.status.idle": "2021-09-27T17:18:49.458624Z",
     "shell.execute_reply": "2021-09-27T17:18:49.458186Z",
     "shell.execute_reply.started": "2021-09-27T16:56:20.547233Z"
    },
    "papermill": {
     "duration": 0.028401,
     "end_time": "2021-09-27T17:18:49.458767",
     "exception": false,
     "start_time": "2021-09-27T17:18:49.430366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "class cfg:\n",
    "    \n",
    "    paths = {\n",
    "        # train path\n",
    "        \"train_csv\"  : \"../input/optiver-realized-volatility-prediction/train.csv\",\n",
    "        \"train_book\" : \"../input/optiver-realized-volatility-prediction/book_train.parquet\",\n",
    "        \"train_trade\": \"../input/optiver-realized-volatility-prediction/trade_train.parquet\",\n",
    "\n",
    "        # test path\n",
    "        \"test_csv\"   : \"../input/optiver-realized-volatility-prediction/test.csv\",\n",
    "        \"test_book\"  : \"../input/optiver-realized-volatility-prediction/book_test.parquet\",\n",
    "        \"test_trade\" : \"../input/optiver-realized-volatility-prediction/trade_test.parquet\",\n",
    "        \n",
    "        'lgb_paths': \"./lgb\"\n",
    "    }\n",
    "\n",
    "    feature_dict_book = {\n",
    "        \"seconds_in_bucket\": [count_unique],\n",
    "        \"wap1\":              [np.sum, np.mean, np.std, np.max],\n",
    "        \"wap2\":              [np.sum, np.mean, np.std, np.max],\n",
    "        \"wap_agg\":           [np.sum, np.mean, np.std, np.max],\n",
    "        \n",
    "        \"iwap1\":             [np.sum, np.mean, np.std, np.max],\n",
    "        \"iwap2\":             [np.sum, np.mean, np.std, np.max],\n",
    "        \"iwap_agg\":          [np.sum, np.mean, np.std, np.max],\n",
    "        \n",
    "        \"log_return1\":       [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        \"log_return2\":       [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        'log_return_agg':    [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        \n",
    "        \"inter_log_return1\": [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        \"inter_log_return2\": [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        'inter_log_return_agg': [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        \n",
    "        \"wap_balance\":       [np.sum, np.mean, np.std, np.max],\n",
    "        \"volume_imbalance\":  [np.sum, np.mean, np.std, np.max],\n",
    "        \"total_volume\":      [np.sum, np.mean, np.std, np.max],\n",
    "        \n",
    "        \"price_spread1\":     [np.sum, np.mean, np.std, np.max],\n",
    "        \"price_spread2\":     [np.sum, np.mean, np.std, np.max],\n",
    "        \"bid_spread\":        [np.sum, np.mean, np.std, np.max],\n",
    "        \"ask_spread\":        [np.sum, np.mean, np.std, np.max],\n",
    "        \n",
    "        'depth':             [np.sum, np.mean, np.std, np.max],\n",
    "        'slope':             [np.sum, np.mean, np.std, np.max],\n",
    "        'dispersion':        [np.sum, np.mean, np.std, np.max],\n",
    "        'price_impact':      [np.sum, np.mean, np.std, np.max],\n",
    "        'ofi':               [np.sum, np.mean, np.std, np.max],\n",
    "        'turn_over':         [np.sum, np.mean, np.std, np.max],\n",
    "    }\n",
    "\n",
    "    feature_dict_book_time = {        \n",
    "        \"log_return1\":       [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "        \"log_return2\":       [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "        \"log_return_agg\":    [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "        \n",
    "        \"inter_log_return1\": [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "        \"inter_log_return2\": [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "        \"inter_log_return_agg\": [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "    }\n",
    "\n",
    "    feature_dict_trade = {\n",
    "        'seconds_in_bucket': [count_unique],       \n",
    "        'log_return':        [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n",
    "        'size':              [np.sum, np.mean, np.std, np.max],\n",
    "        'order_count':       [np.sum, np.mean, np.std, np.max],\n",
    "        'amount':            [np.sum, np.mean, np.std, np.max],\n",
    "    }\n",
    "    \n",
    "    feature_dict_trade_time = {\n",
    "        'log_return':        [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n",
    "        'seconds_in_bucket': [count_unique],\n",
    "        'size':              [np.sum, np.std],\n",
    "        'order_count':       [np.sum, np.std],\n",
    "        'amount':            [np.sum, np.std],\n",
    "    }\n",
    "\n",
    "    model_params = {\n",
    "        \"lgb\": {\n",
    "            'objective': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'max_depth': 8,\n",
    "            'num_leaves': 192, \n",
    "            'max_bin': 255,\n",
    "            'min_data_in_leaf': 750,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.72,\n",
    "            'subsample_freq': 3,\n",
    "            'feature_fraction': 0.5,\n",
    "            'lambda_l1': 0.75,\n",
    "            'lambda_l2': 1.5,\n",
    "            'categorical_column':[0],\n",
    "            'first_metric_only': True,\n",
    "            'n_jobs':-1,\n",
    "            'verbose': -1,\n",
    "            \"seed\": SEED,\n",
    "            \"feature_fraction_seed\": SEED,\n",
    "            \"bagging_seed\": SEED,\n",
    "            \"drop_seed\": SEED,\n",
    "            \"data_random_seed\": SEED,\n",
    "        }\n",
    "    }\n",
    "    bucket_windows = [100, 200, 300, 400, 500]\n",
    "    random_state = SEED\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662f9982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:49.498585Z",
     "iopub.status.busy": "2021-09-27T17:18:49.497877Z",
     "iopub.status.idle": "2021-09-27T17:18:49.500643Z",
     "shell.execute_reply": "2021-09-27T17:18:49.500190Z",
     "shell.execute_reply.started": "2021-09-27T16:56:20.574752Z"
    },
    "papermill": {
     "duration": 0.033382,
     "end_time": "2021-09-27T17:18:49.500785",
     "exception": false,
     "start_time": "2021-09-27T17:18:49.467403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# order book features\n",
    "def get_book_features(file_path):\n",
    "    book_df = pd.read_parquet(file_path)\n",
    "\n",
    "    # calculate wap\n",
    "    book_df['wap1'] = calculate_wap(book_df, rank=\"1\")\n",
    "    book_df['wap2'] = calculate_wap(book_df, rank=\"2\")\n",
    "    book_df[\"wap_agg\"] = calculate_agg_wap(book_df)\n",
    "    \n",
    "    book_df['iwap1'] = calculate_inter_wap(book_df, rank=\"1\")\n",
    "    book_df['iwap2'] = calculate_inter_wap(book_df, rank=\"2\")\n",
    "    book_df[\"iwap_agg\"] = calculate_agg_inter_wap(book_df)\n",
    "\n",
    "    # calculate log return\n",
    "    book_df[\"log_return1\"] = book_df.groupby([\"time_id\"])[\"wap1\"].apply(calculate_log_return)\n",
    "    book_df[\"log_return2\"] = book_df.groupby([\"time_id\"])[\"wap2\"].apply(calculate_log_return)\n",
    "    book_df[\"log_return_agg\"] = book_df.groupby([\"time_id\"])[\"wap_agg\"].apply(calculate_log_return)\n",
    "    \n",
    "    book_df[\"inter_log_return1\"] = book_df.groupby([\"time_id\"])[\"iwap1\"].apply(calculate_log_return)\n",
    "    book_df[\"inter_log_return2\"] = book_df.groupby([\"time_id\"])[\"iwap2\"].apply(calculate_log_return)\n",
    "    book_df[\"inter_log_return_agg\"] = book_df.groupby([\"time_id\"])[\"iwap_agg\"].apply(calculate_log_return)\n",
    "\n",
    "    # calculate balance\n",
    "    book_df[\"wap_balance\"] = abs(book_df[\"wap1\"] - book_df[\"wap2\"])\n",
    "    book_df[\"volume_imbalance\"] = abs(\n",
    "        (book_df[\"ask_size1\"] + book_df[\"ask_size2\"]) - (book_df[\"bid_size1\"] + book_df[\"bid_size2\"]))\n",
    "    book_df[\"total_volume\"] = book_df[\"ask_size1\"] + book_df[\"ask_size2\"] + book_df[\"bid_size1\"] + book_df[\n",
    "        \"bid_size2\"]\n",
    "\n",
    "    # calculate spread\n",
    "    book_df[\"price_spread1\"] = (book_df[\"ask_price1\"] - book_df[\"bid_price1\"]) / (\n",
    "            (book_df[\"ask_price1\"] + book_df[\"bid_price1\"]) / 2)\n",
    "    book_df[\"price_spread2\"] = (book_df[\"ask_price2\"] - book_df[\"bid_price2\"]) / (\n",
    "            (book_df[\"ask_price2\"] + book_df[\"bid_price2\"]) / 2)\n",
    "    book_df[\"bid_spread\"] = book_df[\"bid_price1\"] - book_df[\"bid_price2\"]\n",
    "    book_df[\"ask_spread\"] = book_df[\"ask_price1\"] - book_df[\"ask_price2\"]\n",
    "    \n",
    "    book_df[\"depth\"] = calc_depth(book_df)\n",
    "    book_df[\"slope\"], _ = calc_slope(book_df)\n",
    "    book_df[\"dispersion\"], _ = calc_dispersion(book_df)\n",
    "    book_df[\"price_impact\"], _ = calc_price_impact(book_df)\n",
    "    book_df[\"ofi\"] = calc_ofi(book_df)\n",
    "    book_df[\"turn_over\"] = calc_tt1(book_df)\n",
    "    \n",
    "    book_df_merged = window_stats(book_df, cfg.feature_dict_book, cfg.feature_dict_book_time, cfg.bucket_windows)\n",
    "\n",
    "    book_df_merged[\"row_id\"] = book_df_merged[\"time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    book_df_merged.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return book_df_merged.bfill().ffill()\n",
    "                                                                \n",
    "# trade features\n",
    "def get_trade_price_features(df):\n",
    "    res = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]\n",
    "        vol_tendency = tendency(df_id['price'].values, df_id['size'].values)\n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max = np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min = np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))\n",
    "        energy = np.mean(df_id['price'].values ** 2)\n",
    "        iqr_p = np.percentile(df_id['price'].values, 75) - np.percentile(df_id['price'].values, 25)\n",
    "        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))\n",
    "        energy_v = np.sum(df_id['size'].values ** 2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values, 75) - np.percentile(df_id['size'].values, 25)\n",
    "\n",
    "        res.append({'time_id': n_time_id,\n",
    "                    'tendency': vol_tendency,\n",
    "                    'f_max': f_max,\n",
    "                    'f_min': f_min,\n",
    "                    'df_max': df_max,\n",
    "                    'df_min': df_min,\n",
    "                    'abs_diff': abs_diff,\n",
    "                    'energy': energy,\n",
    "                    'iqr_p': iqr_p,\n",
    "                    'abs_diff_v': abs_diff_v,\n",
    "                    'energy_v': energy_v,\n",
    "                    'iqr_p_v': iqr_p_v})\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "    pass\n",
    "\n",
    "\n",
    "def tau_features(df, sec, weight):\n",
    "    tau_feat = 'tau_' + str(sec)\n",
    "    bucket_col = 'trade_seconds_in_bucket_count_unique_' + str(sec)\n",
    "    df[tau_feat] = np.sqrt(weight/df[bucket_col])\n",
    "\n",
    "    size_feat = 'size_' + str(sec)\n",
    "    order_col = 'trade_order_count_sum_' + str(sec)\n",
    "    df[size_feat] = np.sqrt(weight/df[order_col])\n",
    "\n",
    "    return df\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_trade_features(file_path, buck_windows=cfg.bucket_windows):\n",
    "    trade_df = pd.read_parquet(file_path)\n",
    "\n",
    "    trade_df[\"log_return\"] = trade_df.groupby([\"time_id\"])[\"price\"].apply(calculate_log_return)\n",
    "    trade_df[\"amount\"] = trade_df[\"size\"] * trade_df[\"price\"]\n",
    "\n",
    "    price_features = get_trade_price_features(trade_df)\n",
    "    trade_df_merged = window_stats(trade_df, cfg.feature_dict_trade, cfg.feature_dict_trade_time, buck_windows, additional_dfs=price_features)\n",
    "\n",
    "    trade_df_merged = trade_df_merged.add_prefix(\"trade_\")\n",
    "\n",
    "    trade_df_merged[\"row_id\"] = trade_df_merged[\"trade_time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    trade_df_merged.drop([\"trade_time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    for sec in buck_windows:\n",
    "        trade_df_merged = tau_features(trade_df_merged, sec, weight=sec/600)\n",
    "    return trade_df_merged.bfill().ffill() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc87c4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:49.531903Z",
     "iopub.status.busy": "2021-09-27T17:18:49.531246Z",
     "iopub.status.idle": "2021-09-27T17:18:49.533658Z",
     "shell.execute_reply": "2021-09-27T17:18:49.534097Z",
     "shell.execute_reply.started": "2021-09-27T16:56:20.608352Z"
    },
    "papermill": {
     "duration": 0.024753,
     "end_time": "2021-09-27T17:18:49.534274",
     "exception": false,
     "start_time": "2021-09-27T17:18:49.509521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class GetData:\n",
    "    def __init__(self, df, book_path, trade_path, is_train=True):\n",
    "        self.df = df.copy(deep=True)\n",
    "        self.order_book_path = book_path\n",
    "        self.trade_path = trade_path\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self._get_rowid()\n",
    "\n",
    "    def _get_rowid(self):\n",
    "        self.df[\"row_id\"] = self.df[\"stock_id\"].astype(str) + \"-\" + self.df[\"time_id\"].astype(str)\n",
    "\n",
    "    def get_time_stock(self, buck_windows=cfg.bucket_windows):\n",
    "        vol_cols = []\n",
    "        feat_set = ['log_return1_calculate_rv', 'log_return2_calculate_rv', 'log_return_agg_calculate_rv', 'trade_log_return_calculate_rv']\n",
    "        for feat in feat_set:\n",
    "            for sec in buck_windows:\n",
    "                vol_cols.append(feat + f'_{sec}')\n",
    "        vol_cols += feat_set\n",
    "\n",
    "        df_stock_id = self.df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "        df_time_id = self.df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "        df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "        # Merge with original dataframe\n",
    "        self.df = self.df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "        self.df = self.df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "        self.df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def process_features(self, list_stock_ids):\n",
    "        def parallel_helper(stock_id):\n",
    "            book_sample_path = os.path.join(self.order_book_path, f\"stock_id={stock_id}\")\n",
    "            trade_sample_path = os.path.join(self.trade_path, f\"stock_id={stock_id}\")\n",
    "\n",
    "            return pd.merge(get_book_features(book_sample_path), get_trade_features(trade_sample_path),\n",
    "                            on=\"row_id\",\n",
    "                            how=\"left\")\n",
    "\n",
    "        df = Parallel(n_jobs=-1, verbose=1)(delayed(parallel_helper)(stock_id) for stock_id in list_stock_ids)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _get_features(self):\n",
    "        features_df = self.process_features(self.df[\"stock_id\"].unique())\n",
    "        self.df = self.df.merge(features_df, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "        return self.get_time_stock()\n",
    "        pass\n",
    "\n",
    "    def get_all_features(self, stock_groups):\n",
    "        return create_cluster_aggregations(self._get_features(), stock_groups)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc5212c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:49.566516Z",
     "iopub.status.busy": "2021-09-27T17:18:49.565858Z",
     "iopub.status.idle": "2021-09-27T17:18:49.568827Z",
     "shell.execute_reply": "2021-09-27T17:18:49.569456Z",
     "shell.execute_reply.started": "2021-09-27T16:56:20.628624Z"
    },
    "papermill": {
     "duration": 0.02667,
     "end_time": "2021-09-27T17:18:49.569637",
     "exception": false,
     "start_time": "2021-09-27T17:18:49.542967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, model, is_xgb=True):\n",
    "    y_true = model.get_label()\n",
    "\n",
    "    if is_xgb:\n",
    "        return \"RMSPE\", rmspe(y_true, y_pred)\n",
    "\n",
    "    return \"RMSPE\", rmspe(y_true, y_pred), False\n",
    "\n",
    "def feval_wrapper(y_pred, model):\n",
    "    return feval_rmspe(y_pred, model, is_xgb=False)\n",
    "\n",
    "def get_transform(df, name):\n",
    "    print(f\"\\n[INFO] Using {name} scaler...\\n\")\n",
    "    if name==\"mm\":\n",
    "        scaler = MinMaxScaler().fit(df.drop([\"stock_id\"], axis=1))\n",
    "    elif name==\"mm_11\":\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1)).fit(df.drop([\"stock_id\"], axis=1))\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(df.drop([\"stock_id\"], axis=1))\n",
    "        \n",
    "    df.iloc[:, 1:] = scaler.transform(df.iloc[:, 1:])\n",
    "    pickle.dump(scaler, open(f\"./{name}.pkl\", \"wb\"))\n",
    "    \n",
    "    return df\n",
    "    pass\n",
    "\n",
    "class TrainFer:\n",
    "    def __init__(self, params_dict, n_splits, model_path, random_state):\n",
    "        self.params = params_dict\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.model_path = model_path\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "            \n",
    "    \n",
    "    def train(self, X, y, scaler_name=\"mm\"):\n",
    "        X = get_transform(X, scaler_name)\n",
    "        \n",
    "        oof_predictions = np.zeros(X.shape[0])\n",
    "        kfold = KFold(n_splits=self.n_splits, random_state=0, shuffle=True)\n",
    "        oof_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "            print(f\"\\nFold - {fold}\\n\")\n",
    "\n",
    "            x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "            dtrain = lgb.Dataset(x_train, y_train, weight=1/np.square(y_train))\n",
    "            dval = lgb.Dataset(x_val, y_val, weight=1/np.square(y_val))\n",
    "\n",
    "            model = lgb.train(params=self.params,\n",
    "                              num_boost_round=10000,\n",
    "                              train_set=dtrain,\n",
    "                              valid_sets=[dtrain, dval],\n",
    "                              verbose_eval=250,\n",
    "                              early_stopping_rounds=200,\n",
    "                              feval=feval_wrapper)\n",
    "\n",
    "            fold_preds = model.predict(x_val)\n",
    "            oof_score = rmspe(y_val, fold_preds)\n",
    "            print(f\"\\nRMSPE of fold {fold}: {oof_score}\")\n",
    "            pickle.dump(model, open(os.path.join(self.model_path, f\"lgb_bl_{fold}_{oof_score}.pkl\"), \"wb\"))\n",
    "            \n",
    "            oof_scores.append(oof_score)\n",
    "            oof_predictions[val_idx] = fold_preds\n",
    "        \n",
    "        print(f\"\\nOOF Scores: {oof_scores}\\n\")\n",
    "        rmspe_score = rmspe(y, oof_predictions)\n",
    "        print(f\"OOF RMSPE: {rmspe_score}\")\n",
    "        \n",
    "        return y, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251685d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:18:49.591372Z",
     "iopub.status.busy": "2021-09-27T17:18:49.590284Z",
     "iopub.status.idle": "2021-09-27T19:07:27.006206Z",
     "shell.execute_reply": "2021-09-27T19:07:27.005622Z"
    },
    "papermill": {
     "duration": 6517.427853,
     "end_time": "2021-09-27T19:07:27.006443",
     "exception": false,
     "start_time": "2021-09-27T17:18:49.578590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Using ss scaler...\n",
      "\n",
      "\n",
      "Fold - 0\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421072\ttraining's RMSPE: 0.194815\tvalid_1's rmse: 0.000444662\tvalid_1's RMSPE: 0.206058\n",
      "[500]\ttraining's rmse: 0.000396307\ttraining's RMSPE: 0.183357\tvalid_1's rmse: 0.000430536\tvalid_1's RMSPE: 0.199512\n",
      "[750]\ttraining's rmse: 0.000379898\ttraining's RMSPE: 0.175766\tvalid_1's rmse: 0.000424086\tvalid_1's RMSPE: 0.196523\n",
      "[1000]\ttraining's rmse: 0.000367063\ttraining's RMSPE: 0.169827\tvalid_1's rmse: 0.000420241\tvalid_1's RMSPE: 0.194742\n",
      "[1250]\ttraining's rmse: 0.000356716\ttraining's RMSPE: 0.16504\tvalid_1's rmse: 0.000418758\tvalid_1's RMSPE: 0.194054\n",
      "[1500]\ttraining's rmse: 0.0003469\ttraining's RMSPE: 0.160498\tvalid_1's rmse: 0.000418004\tvalid_1's RMSPE: 0.193705\n",
      "[1750]\ttraining's rmse: 0.000338842\ttraining's RMSPE: 0.15677\tvalid_1's rmse: 0.000417451\tvalid_1's RMSPE: 0.193448\n",
      "[2000]\ttraining's rmse: 0.000331213\ttraining's RMSPE: 0.153241\tvalid_1's rmse: 0.000417176\tvalid_1's RMSPE: 0.193321\n",
      "[2250]\ttraining's rmse: 0.000324101\ttraining's RMSPE: 0.14995\tvalid_1's rmse: 0.000417343\tvalid_1's RMSPE: 0.193399\n",
      "Early stopping, best iteration is:\n",
      "[2112]\ttraining's rmse: 0.000327949\ttraining's RMSPE: 0.151731\tvalid_1's rmse: 0.000417074\tvalid_1's RMSPE: 0.193274\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 0: 0.19327362714270208\n",
      "\n",
      "Fold - 1\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.0004217\ttraining's RMSPE: 0.195071\tvalid_1's rmse: 0.000444147\tvalid_1's RMSPE: 0.205965\n",
      "[500]\ttraining's rmse: 0.000396569\ttraining's RMSPE: 0.183446\tvalid_1's rmse: 0.000430796\tvalid_1's RMSPE: 0.199774\n",
      "[750]\ttraining's rmse: 0.000380194\ttraining's RMSPE: 0.175871\tvalid_1's rmse: 0.000423963\tvalid_1's RMSPE: 0.196605\n",
      "[1000]\ttraining's rmse: 0.000367552\ttraining's RMSPE: 0.170023\tvalid_1's rmse: 0.000420386\tvalid_1's RMSPE: 0.194946\n",
      "[1250]\ttraining's rmse: 0.000357078\ttraining's RMSPE: 0.165178\tvalid_1's rmse: 0.000419318\tvalid_1's RMSPE: 0.194451\n",
      "[1500]\ttraining's rmse: 0.000347895\ttraining's RMSPE: 0.16093\tvalid_1's rmse: 0.000418953\tvalid_1's RMSPE: 0.194282\n",
      "[1750]\ttraining's rmse: 0.000339586\ttraining's RMSPE: 0.157087\tvalid_1's rmse: 0.000418215\tvalid_1's RMSPE: 0.193939\n",
      "[2000]\ttraining's rmse: 0.000332063\ttraining's RMSPE: 0.153607\tvalid_1's rmse: 0.000418163\tvalid_1's RMSPE: 0.193915\n",
      "Early stopping, best iteration is:\n",
      "[1947]\ttraining's rmse: 0.000333663\ttraining's RMSPE: 0.154347\tvalid_1's rmse: 0.000417725\tvalid_1's RMSPE: 0.193712\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 1: 0.1937121956037605\n",
      "\n",
      "Fold - 2\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000420773\ttraining's RMSPE: 0.194834\tvalid_1's rmse: 0.000448976\tvalid_1's RMSPE: 0.207386\n",
      "[500]\ttraining's rmse: 0.000394709\ttraining's RMSPE: 0.182765\tvalid_1's rmse: 0.000435961\tvalid_1's RMSPE: 0.201374\n",
      "[750]\ttraining's rmse: 0.000378545\ttraining's RMSPE: 0.175281\tvalid_1's rmse: 0.000430536\tvalid_1's RMSPE: 0.198869\n",
      "[1000]\ttraining's rmse: 0.000366339\ttraining's RMSPE: 0.169629\tvalid_1's rmse: 0.000428931\tvalid_1's RMSPE: 0.198128\n",
      "[1250]\ttraining's rmse: 0.000356071\ttraining's RMSPE: 0.164875\tvalid_1's rmse: 0.000428768\tvalid_1's RMSPE: 0.198052\n",
      "[1500]\ttraining's rmse: 0.000346681\ttraining's RMSPE: 0.160527\tvalid_1's rmse: 0.000428647\tvalid_1's RMSPE: 0.197996\n",
      "Early stopping, best iteration is:\n",
      "[1394]\ttraining's rmse: 0.000350503\ttraining's RMSPE: 0.162296\tvalid_1's rmse: 0.000428156\tvalid_1's RMSPE: 0.197769\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 2: 0.19776919518300184\n",
      "\n",
      "Fold - 3\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421909\ttraining's RMSPE: 0.195331\tvalid_1's rmse: 0.000447838\tvalid_1's RMSPE: 0.206984\n",
      "[500]\ttraining's rmse: 0.000395455\ttraining's RMSPE: 0.183083\tvalid_1's rmse: 0.000432479\tvalid_1's RMSPE: 0.199885\n",
      "[750]\ttraining's rmse: 0.000379985\ttraining's RMSPE: 0.175921\tvalid_1's rmse: 0.000427189\tvalid_1's RMSPE: 0.19744\n",
      "[1000]\ttraining's rmse: 0.000367105\ttraining's RMSPE: 0.169958\tvalid_1's rmse: 0.000424208\tvalid_1's RMSPE: 0.196062\n",
      "[1250]\ttraining's rmse: 0.000356576\ttraining's RMSPE: 0.165084\tvalid_1's rmse: 0.000423648\tvalid_1's RMSPE: 0.195804\n",
      "[1500]\ttraining's rmse: 0.000347199\ttraining's RMSPE: 0.160743\tvalid_1's rmse: 0.000422732\tvalid_1's RMSPE: 0.19538\n",
      "[1750]\ttraining's rmse: 0.000338572\ttraining's RMSPE: 0.156749\tvalid_1's rmse: 0.000422399\tvalid_1's RMSPE: 0.195226\n",
      "[2000]\ttraining's rmse: 0.000330757\ttraining's RMSPE: 0.15313\tvalid_1's rmse: 0.000422228\tvalid_1's RMSPE: 0.195147\n",
      "[2250]\ttraining's rmse: 0.000323903\ttraining's RMSPE: 0.149957\tvalid_1's rmse: 0.000422201\tvalid_1's RMSPE: 0.195135\n",
      "Early stopping, best iteration is:\n",
      "[2051]\ttraining's rmse: 0.000329303\ttraining's RMSPE: 0.152457\tvalid_1's rmse: 0.000422126\tvalid_1's RMSPE: 0.1951\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 3: 0.19509999892129598\n",
      "\n",
      "Fold - 4\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000420974\ttraining's RMSPE: 0.19483\tvalid_1's rmse: 0.000477273\tvalid_1's RMSPE: 0.220897\n",
      "[500]\ttraining's rmse: 0.000395455\ttraining's RMSPE: 0.183019\tvalid_1's rmse: 0.000461913\tvalid_1's RMSPE: 0.213788\n",
      "[750]\ttraining's rmse: 0.000379205\ttraining's RMSPE: 0.175499\tvalid_1's rmse: 0.000456513\tvalid_1's RMSPE: 0.211289\n",
      "[1000]\ttraining's rmse: 0.000366049\ttraining's RMSPE: 0.16941\tvalid_1's rmse: 0.000453557\tvalid_1's RMSPE: 0.209921\n",
      "[1250]\ttraining's rmse: 0.000355374\ttraining's RMSPE: 0.16447\tvalid_1's rmse: 0.000452917\tvalid_1's RMSPE: 0.209625\n",
      "Early stopping, best iteration is:\n",
      "[1262]\ttraining's rmse: 0.000354863\ttraining's RMSPE: 0.164233\tvalid_1's rmse: 0.000452803\tvalid_1's RMSPE: 0.209572\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 4: 0.20957158537951318\n",
      "\n",
      "OOF Scores: [0.19327362714270208, 0.1937121956037605, 0.19776919518300184, 0.19509999892129598, 0.20957158537951318]\n",
      "\n",
      "OOF RMSPE: 0.1979777675824728\n",
      "\n",
      "[INFO] Using mm scaler...\n",
      "\n",
      "\n",
      "Fold - 0\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421801\ttraining's RMSPE: 0.195152\tvalid_1's rmse: 0.000445646\tvalid_1's RMSPE: 0.206514\n",
      "[500]\ttraining's rmse: 0.000397092\ttraining's RMSPE: 0.18372\tvalid_1's rmse: 0.000432172\tvalid_1's RMSPE: 0.20027\n",
      "[750]\ttraining's rmse: 0.000380713\ttraining's RMSPE: 0.176142\tvalid_1's rmse: 0.00042549\tvalid_1's RMSPE: 0.197174\n",
      "[1000]\ttraining's rmse: 0.000367791\ttraining's RMSPE: 0.170164\tvalid_1's rmse: 0.000421474\tvalid_1's RMSPE: 0.195313\n",
      "[1250]\ttraining's rmse: 0.00035711\ttraining's RMSPE: 0.165222\tvalid_1's rmse: 0.000420317\tvalid_1's RMSPE: 0.194777\n",
      "[1500]\ttraining's rmse: 0.00034755\ttraining's RMSPE: 0.160799\tvalid_1's rmse: 0.000418876\tvalid_1's RMSPE: 0.194109\n",
      "[1750]\ttraining's rmse: 0.000339382\ttraining's RMSPE: 0.15702\tvalid_1's rmse: 0.000418154\tvalid_1's RMSPE: 0.193774\n",
      "[2000]\ttraining's rmse: 0.000331914\ttraining's RMSPE: 0.153565\tvalid_1's rmse: 0.000417842\tvalid_1's RMSPE: 0.19363\n",
      "Early stopping, best iteration is:\n",
      "[1879]\ttraining's rmse: 0.000335507\ttraining's RMSPE: 0.155227\tvalid_1's rmse: 0.000417631\tvalid_1's RMSPE: 0.193532\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 0: 0.19353180168594394\n",
      "\n",
      "Fold - 1\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421835\ttraining's RMSPE: 0.195134\tvalid_1's rmse: 0.000443514\tvalid_1's RMSPE: 0.205671\n",
      "[500]\ttraining's rmse: 0.000396014\ttraining's RMSPE: 0.183189\tvalid_1's rmse: 0.000429484\tvalid_1's RMSPE: 0.199165\n",
      "[750]\ttraining's rmse: 0.00037976\ttraining's RMSPE: 0.17567\tvalid_1's rmse: 0.000423328\tvalid_1's RMSPE: 0.196311\n",
      "[1000]\ttraining's rmse: 0.000367347\ttraining's RMSPE: 0.169929\tvalid_1's rmse: 0.000420393\tvalid_1's RMSPE: 0.194949\n",
      "[1250]\ttraining's rmse: 0.000356387\ttraining's RMSPE: 0.164859\tvalid_1's rmse: 0.000419057\tvalid_1's RMSPE: 0.19433\n",
      "[1500]\ttraining's rmse: 0.000346827\ttraining's RMSPE: 0.160436\tvalid_1's rmse: 0.00041857\tvalid_1's RMSPE: 0.194104\n",
      "Early stopping, best iteration is:\n",
      "[1450]\ttraining's rmse: 0.000348603\ttraining's RMSPE: 0.161258\tvalid_1's rmse: 0.000418156\tvalid_1's RMSPE: 0.193912\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 1: 0.193911865490142\n",
      "\n",
      "Fold - 2\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421284\ttraining's RMSPE: 0.195071\tvalid_1's rmse: 0.000449192\tvalid_1's RMSPE: 0.207486\n",
      "[500]\ttraining's rmse: 0.000394897\ttraining's RMSPE: 0.182852\tvalid_1's rmse: 0.000435615\tvalid_1's RMSPE: 0.201215\n",
      "[750]\ttraining's rmse: 0.000378467\ttraining's RMSPE: 0.175245\tvalid_1's rmse: 0.000430201\tvalid_1's RMSPE: 0.198714\n",
      "[1000]\ttraining's rmse: 0.000366218\ttraining's RMSPE: 0.169573\tvalid_1's rmse: 0.000428435\tvalid_1's RMSPE: 0.197898\n",
      "[1250]\ttraining's rmse: 0.00035575\ttraining's RMSPE: 0.164726\tvalid_1's rmse: 0.000428175\tvalid_1's RMSPE: 0.197778\n",
      "[1500]\ttraining's rmse: 0.000346763\ttraining's RMSPE: 0.160564\tvalid_1's rmse: 0.000428398\tvalid_1's RMSPE: 0.197881\n",
      "Early stopping, best iteration is:\n",
      "[1372]\ttraining's rmse: 0.000351107\ttraining's RMSPE: 0.162576\tvalid_1's rmse: 0.000427547\tvalid_1's RMSPE: 0.197488\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 2: 0.1974880985058724\n",
      "\n",
      "Fold - 3\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000420909\ttraining's RMSPE: 0.194868\tvalid_1's rmse: 0.000445608\tvalid_1's RMSPE: 0.205953\n",
      "[500]\ttraining's rmse: 0.000395253\ttraining's RMSPE: 0.18299\tvalid_1's rmse: 0.000430925\tvalid_1's RMSPE: 0.199167\n",
      "[750]\ttraining's rmse: 0.000379563\ttraining's RMSPE: 0.175726\tvalid_1's rmse: 0.000425285\tvalid_1's RMSPE: 0.19656\n",
      "[1000]\ttraining's rmse: 0.000366582\ttraining's RMSPE: 0.169717\tvalid_1's rmse: 0.000421525\tvalid_1's RMSPE: 0.194822\n",
      "[1250]\ttraining's rmse: 0.000355919\ttraining's RMSPE: 0.16478\tvalid_1's rmse: 0.00042007\tvalid_1's RMSPE: 0.19415\n",
      "[1500]\ttraining's rmse: 0.000346764\ttraining's RMSPE: 0.160541\tvalid_1's rmse: 0.000419488\tvalid_1's RMSPE: 0.19388\n",
      "[1750]\ttraining's rmse: 0.000338064\ttraining's RMSPE: 0.156513\tvalid_1's rmse: 0.000418504\tvalid_1's RMSPE: 0.193426\n",
      "[2000]\ttraining's rmse: 0.000330406\ttraining's RMSPE: 0.152968\tvalid_1's rmse: 0.000418447\tvalid_1's RMSPE: 0.1934\n",
      "Early stopping, best iteration is:\n",
      "[1857]\ttraining's rmse: 0.000334884\ttraining's RMSPE: 0.155041\tvalid_1's rmse: 0.000418305\tvalid_1's RMSPE: 0.193334\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 3: 0.1933340968369823\n",
      "\n",
      "Fold - 4\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000420607\ttraining's RMSPE: 0.19466\tvalid_1's rmse: 0.000480563\tvalid_1's RMSPE: 0.22242\n",
      "[500]\ttraining's rmse: 0.000394781\ttraining's RMSPE: 0.182708\tvalid_1's rmse: 0.000465701\tvalid_1's RMSPE: 0.215541\n",
      "[750]\ttraining's rmse: 0.000378953\ttraining's RMSPE: 0.175382\tvalid_1's rmse: 0.000462117\tvalid_1's RMSPE: 0.213882\n",
      "[1000]\ttraining's rmse: 0.000366168\ttraining's RMSPE: 0.169466\tvalid_1's rmse: 0.000459035\tvalid_1's RMSPE: 0.212456\n",
      "[1250]\ttraining's rmse: 0.000355692\ttraining's RMSPE: 0.164617\tvalid_1's rmse: 0.000456525\tvalid_1's RMSPE: 0.211294\n",
      "[1500]\ttraining's rmse: 0.000346465\ttraining's RMSPE: 0.160347\tvalid_1's rmse: 0.000455701\tvalid_1's RMSPE: 0.210913\n",
      "Early stopping, best iteration is:\n",
      "[1465]\ttraining's rmse: 0.000347646\ttraining's RMSPE: 0.160893\tvalid_1's rmse: 0.000455665\tvalid_1's RMSPE: 0.210896\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 4: 0.21089633950997821\n",
      "\n",
      "OOF Scores: [0.19353180168594394, 0.193911865490142, 0.1974880985058724, 0.1933340968369823, 0.21089633950997821]\n",
      "\n",
      "OOF RMSPE: 0.19794606302720208\n",
      "\n",
      "[INFO] Using mm_11 scaler...\n",
      "\n",
      "\n",
      "Fold - 0\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421345\ttraining's RMSPE: 0.194941\tvalid_1's rmse: 0.000444817\tvalid_1's RMSPE: 0.20613\n",
      "[500]\ttraining's rmse: 0.000396976\ttraining's RMSPE: 0.183667\tvalid_1's rmse: 0.000431818\tvalid_1's RMSPE: 0.200106\n",
      "[750]\ttraining's rmse: 0.000380729\ttraining's RMSPE: 0.17615\tvalid_1's rmse: 0.000425138\tvalid_1's RMSPE: 0.197011\n",
      "[1000]\ttraining's rmse: 0.000367642\ttraining's RMSPE: 0.170095\tvalid_1's rmse: 0.000420991\tvalid_1's RMSPE: 0.195089\n",
      "[1250]\ttraining's rmse: 0.000357048\ttraining's RMSPE: 0.165194\tvalid_1's rmse: 0.000419519\tvalid_1's RMSPE: 0.194407\n",
      "[1500]\ttraining's rmse: 0.000347672\ttraining's RMSPE: 0.160855\tvalid_1's rmse: 0.000418325\tvalid_1's RMSPE: 0.193854\n",
      "[1750]\ttraining's rmse: 0.00033957\ttraining's RMSPE: 0.157107\tvalid_1's rmse: 0.000417756\tvalid_1's RMSPE: 0.19359\n",
      "Early stopping, best iteration is:\n",
      "[1674]\ttraining's rmse: 0.000341937\ttraining's RMSPE: 0.158202\tvalid_1's rmse: 0.000417384\tvalid_1's RMSPE: 0.193418\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 0: 0.19341764006753545\n",
      "\n",
      "Fold - 1\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421914\ttraining's RMSPE: 0.19517\tvalid_1's rmse: 0.000444031\tvalid_1's RMSPE: 0.205911\n",
      "[500]\ttraining's rmse: 0.000396356\ttraining's RMSPE: 0.183348\tvalid_1's rmse: 0.000430228\tvalid_1's RMSPE: 0.19951\n",
      "[750]\ttraining's rmse: 0.000379984\ttraining's RMSPE: 0.175774\tvalid_1's rmse: 0.00042465\tvalid_1's RMSPE: 0.196923\n",
      "[1000]\ttraining's rmse: 0.000367126\ttraining's RMSPE: 0.169826\tvalid_1's rmse: 0.000421259\tvalid_1's RMSPE: 0.195351\n",
      "[1250]\ttraining's rmse: 0.000356497\ttraining's RMSPE: 0.164909\tvalid_1's rmse: 0.000419624\tvalid_1's RMSPE: 0.194593\n",
      "[1500]\ttraining's rmse: 0.000347154\ttraining's RMSPE: 0.160588\tvalid_1's rmse: 0.000420014\tvalid_1's RMSPE: 0.194774\n",
      "Early stopping, best iteration is:\n",
      "[1422]\ttraining's rmse: 0.00034979\ttraining's RMSPE: 0.161807\tvalid_1's rmse: 0.000419384\tvalid_1's RMSPE: 0.194481\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 1: 0.19448127902379495\n",
      "\n",
      "Fold - 2\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000420863\ttraining's RMSPE: 0.194876\tvalid_1's rmse: 0.000449548\tvalid_1's RMSPE: 0.207651\n",
      "[500]\ttraining's rmse: 0.000394901\ttraining's RMSPE: 0.182854\tvalid_1's rmse: 0.000436311\tvalid_1's RMSPE: 0.201536\n",
      "[750]\ttraining's rmse: 0.00037852\ttraining's RMSPE: 0.175269\tvalid_1's rmse: 0.000432236\tvalid_1's RMSPE: 0.199654\n",
      "[1000]\ttraining's rmse: 0.000365972\ttraining's RMSPE: 0.169459\tvalid_1's rmse: 0.000431036\tvalid_1's RMSPE: 0.1991\n",
      "[1250]\ttraining's rmse: 0.000355363\ttraining's RMSPE: 0.164546\tvalid_1's rmse: 0.000429905\tvalid_1's RMSPE: 0.198577\n",
      "[1500]\ttraining's rmse: 0.000346119\ttraining's RMSPE: 0.160266\tvalid_1's rmse: 0.000429435\tvalid_1's RMSPE: 0.19836\n",
      "Early stopping, best iteration is:\n",
      "[1398]\ttraining's rmse: 0.000349611\ttraining's RMSPE: 0.161883\tvalid_1's rmse: 0.000429052\tvalid_1's RMSPE: 0.198183\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 2: 0.1981831127593652\n",
      "\n",
      "Fold - 3\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000421662\ttraining's RMSPE: 0.195217\tvalid_1's rmse: 0.000447178\tvalid_1's RMSPE: 0.206679\n",
      "[500]\ttraining's rmse: 0.000395855\ttraining's RMSPE: 0.183269\tvalid_1's rmse: 0.00043285\tvalid_1's RMSPE: 0.200057\n",
      "[750]\ttraining's rmse: 0.00037962\ttraining's RMSPE: 0.175752\tvalid_1's rmse: 0.000426987\tvalid_1's RMSPE: 0.197347\n",
      "[1000]\ttraining's rmse: 0.000366552\ttraining's RMSPE: 0.169702\tvalid_1's rmse: 0.000423637\tvalid_1's RMSPE: 0.195798\n",
      "[1250]\ttraining's rmse: 0.000356251\ttraining's RMSPE: 0.164934\tvalid_1's rmse: 0.000422088\tvalid_1's RMSPE: 0.195082\n",
      "[1500]\ttraining's rmse: 0.000347022\ttraining's RMSPE: 0.160661\tvalid_1's rmse: 0.000421037\tvalid_1's RMSPE: 0.194597\n",
      "[1750]\ttraining's rmse: 0.000338419\ttraining's RMSPE: 0.156678\tvalid_1's rmse: 0.000420423\tvalid_1's RMSPE: 0.194313\n",
      "Early stopping, best iteration is:\n",
      "[1728]\ttraining's rmse: 0.000339009\ttraining's RMSPE: 0.156951\tvalid_1's rmse: 0.000420181\tvalid_1's RMSPE: 0.194201\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 3: 0.19420084506378646\n",
      "\n",
      "Fold - 4\n",
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000419688\ttraining's RMSPE: 0.194235\tvalid_1's rmse: 0.000482756\tvalid_1's RMSPE: 0.223435\n",
      "[500]\ttraining's rmse: 0.000394125\ttraining's RMSPE: 0.182404\tvalid_1's rmse: 0.000467034\tvalid_1's RMSPE: 0.216158\n",
      "[750]\ttraining's rmse: 0.000378058\ttraining's RMSPE: 0.174968\tvalid_1's rmse: 0.000461257\tvalid_1's RMSPE: 0.213485\n",
      "[1000]\ttraining's rmse: 0.000365696\ttraining's RMSPE: 0.169247\tvalid_1's rmse: 0.000458312\tvalid_1's RMSPE: 0.212121\n",
      "[1250]\ttraining's rmse: 0.000355132\ttraining's RMSPE: 0.164358\tvalid_1's rmse: 0.000455477\tvalid_1's RMSPE: 0.210809\n",
      "[1500]\ttraining's rmse: 0.000346227\ttraining's RMSPE: 0.160236\tvalid_1's rmse: 0.000455176\tvalid_1's RMSPE: 0.21067\n",
      "[1750]\ttraining's rmse: 0.000337821\ttraining's RMSPE: 0.156346\tvalid_1's rmse: 0.000454732\tvalid_1's RMSPE: 0.210465\n",
      "[2000]\ttraining's rmse: 0.000330028\ttraining's RMSPE: 0.152739\tvalid_1's rmse: 0.000454576\tvalid_1's RMSPE: 0.210392\n",
      "Early stopping, best iteration is:\n",
      "[1884]\ttraining's rmse: 0.000333526\ttraining's RMSPE: 0.154359\tvalid_1's rmse: 0.000454081\tvalid_1's RMSPE: 0.210163\n",
      "Evaluated only: rmse\n",
      "\n",
      "RMSPE of fold 4: 0.21016336158674848\n",
      "\n",
      "OOF Scores: [0.19341764006753545, 0.19448127902379495, 0.1981831127593652, 0.19420084506378646, 0.21016336158674848]\n",
      "\n",
      "OOF RMSPE: 0.1981880246617465\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    train_feats = pickle.load(open(\"../input/processed-dataset-orvp/train_df.pkl\", \"rb\"))\n",
    "    model = TrainFer(cfg.model_params[\"lgb\"], n_splits=5, model_path=cfg.paths[\"lgb_paths\"], random_state=cfg.random_state) \n",
    "    \n",
    "    y_targets, oof_preds_ss = model.train(train_feats.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_feats[\"target\"], \"ss\")\n",
    "    _, oof_preds_mm = model.train(train_feats.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_feats[\"target\"], \"mm\")\n",
    "    _, oof_preds_mm_11 = model.train(train_feats.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_feats[\"target\"], \"mm_11\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101d148c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:07:27.110355Z",
     "iopub.status.busy": "2021-09-27T19:07:27.109712Z",
     "iopub.status.idle": "2021-09-27T19:07:27.124210Z",
     "shell.execute_reply": "2021-09-27T19:07:27.124643Z"
    },
    "papermill": {
     "duration": 0.071143,
     "end_time": "2021-09-27T19:07:27.124825",
     "exception": false,
     "start_time": "2021-09-27T19:07:27.053682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19672923096551106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_weights = 0.33 * oof_preds_ss + 0.33 * oof_preds_mm + 0.33 * oof_preds_mm_11\n",
    "rmspe(y_targets, equal_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f5c9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:07:27.223948Z",
     "iopub.status.busy": "2021-09-27T19:07:27.223079Z",
     "iopub.status.idle": "2021-09-27T19:07:27.737200Z",
     "shell.execute_reply": "2021-09-27T19:07:27.736698Z"
    },
    "papermill": {
     "duration": 0.564166,
     "end_time": "2021-09-27T19:07:27.737334",
     "exception": false,
     "start_time": "2021-09-27T19:07:27.173168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196661\n",
      "         Iterations: 18\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 23\n",
      "Weights arit: [0.38004045 0.34412447 0.26095594]\n"
     ]
    }
   ],
   "source": [
    "# optimized weights\n",
    "def minimize_arit(W):\n",
    "    model_preds = W[0] * oof_preds_ss + W[1] * oof_preds_mm + W[2] * oof_preds_mm_11 \n",
    "    return rmspe(y_targets, model_preds)\n",
    "\n",
    "W0 = minimize(minimize_arit, [1./3]*3, options={'gtol': 1e-6, 'disp': True}).x\n",
    "print('Weights arit:', W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c0ac2f",
   "metadata": {
    "papermill": {
     "duration": 0.049503,
     "end_time": "2021-09-27T19:07:27.834299",
     "exception": false,
     "start_time": "2021-09-27T19:07:27.784796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6529.57945,
   "end_time": "2021-09-27T19:07:29.902837",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T17:18:40.323387",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
