{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426a54e3",
   "metadata": {
    "papermill": {
     "duration": 0.008781,
     "end_time": "2021-09-25T21:00:25.402709",
     "exception": false,
     "start_time": "2021-09-25T21:00:25.393928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8e1c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:25.438387Z",
     "iopub.status.busy": "2021-09-25T21:00:25.429482Z",
     "iopub.status.idle": "2021-09-25T21:00:52.614367Z",
     "shell.execute_reply": "2021-09-25T21:00:52.613727Z",
     "shell.execute_reply.started": "2021-09-25T20:54:53.609774Z"
    },
    "papermill": {
     "duration": 27.203702,
     "end_time": "2021-09-25T21:00:52.614526",
     "exception": false,
     "start_time": "2021-09-25T21:00:25.410824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (0.23.2)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (1.6.3)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (4.61.1)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (1.19.5)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (1.7.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (2.1.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (0.6)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-3.1.1\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a01190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:52.644301Z",
     "iopub.status.busy": "2021-09-25T21:00:52.643200Z",
     "iopub.status.idle": "2021-09-25T21:00:54.767538Z",
     "shell.execute_reply": "2021-09-25T21:00:54.766648Z",
     "shell.execute_reply.started": "2021-09-25T20:55:20.860404Z"
    },
    "papermill": {
     "duration": 2.14309,
     "end_time": "2021-09-25T21:00:54.767689",
     "exception": false,
     "start_time": "2021-09-25T21:00:52.624599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 2021\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27425865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:54.806644Z",
     "iopub.status.busy": "2021-09-25T21:00:54.804955Z",
     "iopub.status.idle": "2021-09-25T21:00:54.807283Z",
     "shell.execute_reply": "2021-09-25T21:00:54.807676Z",
     "shell.execute_reply.started": "2021-09-25T20:55:23.055458Z"
    },
    "papermill": {
     "duration": 0.03071,
     "end_time": "2021-09-25T21:00:54.807803",
     "exception": false,
     "start_time": "2021-09-25T21:00:54.777093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature utils\n",
    "def calculate_wap(df, rank=\"1\"):\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"bid_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n",
    "                df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "\n",
    "\n",
    "def calculate_inter_wap(df, rank=\"1\"):\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n",
    "            df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def calculate_rv(series):\n",
    "    return np.sqrt(np.sum(np.square(series)))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def get_stats_window(df, seconds_in_bucket, features_dict, add_suffix=False):\n",
    "    df_feature = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(features_dict).reset_index()\n",
    "    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n",
    "\n",
    "    if add_suffix:\n",
    "        df_feature = df_feature.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "\n",
    "    return df_feature\n",
    "    pass\n",
    "\n",
    "\n",
    "def window_stats(df, feature_dict, feature_dict_time, second_windows, additional_dfs=None):\n",
    "    df_merged = get_stats_window(df, seconds_in_bucket=0, features_dict=feature_dict)\n",
    "\n",
    "    if additional_dfs is not None:\n",
    "        df_merged = df_merged.merge(additional_dfs, how='left', left_on='time_id_', right_on='time_id')\n",
    "\n",
    "    temp_dfs = []\n",
    "    for window in second_windows:\n",
    "        temp_dfs.append(\n",
    "            (window,\n",
    "             get_stats_window(df, seconds_in_bucket=window, features_dict=feature_dict_time, add_suffix=True)\n",
    "             )\n",
    "        )\n",
    "\n",
    "    for window, temp_df in temp_dfs:\n",
    "        df_merged = df_merged.merge(temp_df, how=\"left\", left_on=\"time_id_\", right_on=f\"time_id__{window}\")\n",
    "        df_merged.drop(columns=[f\"time_id__{window}\"], inplace=True)\n",
    "\n",
    "    return df_merged\n",
    "    pass\n",
    "\n",
    "\n",
    "def tendency(price, vol):\n",
    "    diff = np.diff(price)\n",
    "    val = (diff / price[1:]) * 100\n",
    "    power = np.sum(val * vol[1:])\n",
    "    return power\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_stock_clusters(df, n_clusters=7):\n",
    "    pivoted_data = df.pivot(index=\"time_id\", columns=[\"stock_id\"], values=\"target\")\n",
    "    corr_pivoted = pivoted_data.corr()\n",
    "\n",
    "    clusters = KMeans(n_clusters, random_state=cfg.random_state).fit(corr_pivoted.values)\n",
    "\n",
    "    groups = []\n",
    "    for i in range(n_clusters):\n",
    "        groups.append([x-1] for x in (corr_pivoted.index+1)*(clusters.labels_ == i) if x > 0)\n",
    "    return groups\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_cluster_aggregations(df, groups):\n",
    "    feats = []\n",
    "\n",
    "    for i, idx in enumerate(groups):\n",
    "        chunk_df = df.loc[df['stock_id'].isin(idx)]\n",
    "        chunk_df = chunk_df.groupby(['time_id']).agg(np.nanmean)\n",
    "        chunk_df.loc[:, 'stock_id'] = str(i) + 'c1'\n",
    "        feats.append(chunk_df)\n",
    "\n",
    "    feats = pd.concat(feats).reset_index()\n",
    "    if \"target\" in feats.columns:\n",
    "        feats.drop(columns=['target'], inplace=True)\n",
    "\n",
    "    feats = feats.pivot(index='time_id', columns='stock_id')\n",
    "    feats.columns = [\"_\".join(x) for x in feats.columns.ravel()]\n",
    "    feats.reset_index(inplace=True)\n",
    "\n",
    "    return pd.merge(df, feats, how=\"left\", on=\"time_id\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b586e0fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:54.843258Z",
     "iopub.status.busy": "2021-09-25T21:00:54.841916Z",
     "iopub.status.idle": "2021-09-25T21:00:54.844812Z",
     "shell.execute_reply": "2021-09-25T21:00:54.844353Z",
     "shell.execute_reply.started": "2021-09-25T20:55:23.078096Z"
    },
    "papermill": {
     "duration": 0.027908,
     "end_time": "2021-09-25T21:00:54.844921",
     "exception": false,
     "start_time": "2021-09-25T21:00:54.817013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "class cfg:\n",
    "    \n",
    "    paths = {\n",
    "        # train path\n",
    "        \"train_csv\"  : \"../input/optiver-realized-volatility-prediction/train.csv\",\n",
    "        \"train_book\" : \"../input/optiver-realized-volatility-prediction/book_train.parquet\",\n",
    "        \"train_trade\": \"../input/optiver-realized-volatility-prediction/trade_train.parquet\",\n",
    "\n",
    "        # test path\n",
    "        \"test_csv\"   : \"../input/optiver-realized-volatility-prediction/test.csv\",\n",
    "        \"test_book\"  : \"../input/optiver-realized-volatility-prediction/book_test.parquet\",\n",
    "        \"test_trade\" : \"../input/optiver-realized-volatility-prediction/trade_test.parquet\",\n",
    "        \n",
    "        # model paths\n",
    "        \"tabnet\": \"./tabnet\"\n",
    "    }\n",
    "\n",
    "    feature_dict_book = {\n",
    "        \"seconds_in_bucket\": [count_unique],\n",
    "        \"wap1\":              [np.sum, np.mean, np.std, np.max],\n",
    "        \"wap2\":              [np.sum, np.mean, np.std, np.max],\n",
    "        \"iwap1\":             [np.sum, np.mean, np.std, np.max],\n",
    "        \"iwap2\":             [np.sum, np.mean, np.std, np.max],\n",
    "        \n",
    "        \"log_return1\":       [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"log_return2\":       [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"inter_log_return1\": [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"inter_log_return2\": [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \n",
    "        \"wap_balance\":       [np.sum, np.mean, np.std, np.max],\n",
    "        \"volume_imbalance\":  [np.sum, np.mean, np.std, np.max],\n",
    "        \"total_volume\":      [np.sum, np.mean, np.std, np.max],\n",
    "        \n",
    "        \"price_spread1\":     [np.sum, np.mean, np.std, np.max],\n",
    "        \"price_spread2\":     [np.sum, np.mean, np.std, np.max],\n",
    "        \"bid_spread\":        [np.sum, np.mean, np.std, np.max],\n",
    "        \"ask_spread\":        [np.sum, np.mean, np.std, np.max],\n",
    "    }\n",
    "\n",
    "    feature_dict_book_time = {        \n",
    "        \"log_return1\":       [calculate_rv],\n",
    "        \"log_return2\":       [calculate_rv],\n",
    "        \"inter_log_return1\": [calculate_rv],\n",
    "        \"inter_log_return2\": [calculate_rv],\n",
    "    }\n",
    "\n",
    "    feature_dict_trade = {\n",
    "        'seconds_in_bucket': [count_unique],       \n",
    "        'log_return':        [np.sum, calculate_rv, np.mean, np.std],\n",
    "        'size':              [np.sum, np.mean, np.std, np.max],\n",
    "        'order_count':       [np.sum, np.mean, np.std, np.max],\n",
    "        'amount':            [np.sum, np.mean, np.std, np.max],\n",
    "    }\n",
    "    \n",
    "    feature_dict_trade_time = {\n",
    "        'log_return':        [calculate_rv],\n",
    "        'seconds_in_bucket': [count_unique],\n",
    "        'size':              [np.sum, np.std],\n",
    "        'order_count':       [np.sum, np.std],\n",
    "        'amount':            [np.sum, np.std],\n",
    "    }\n",
    "\n",
    "    model_params = {\n",
    "        \"tabnet\": {\n",
    "            \"cat_idxs\"    : [0],\n",
    "            \"cat_dims\"   : [112],\n",
    "            \"cat_emb_dim\": 1,\n",
    "            \"n_d\"        : 16,\n",
    "            \"n_a\"        : 16,\n",
    "            \"n_steps\"    : 2,\n",
    "            \"gamma\"      : 2,\n",
    "            \"n_independent\": 2,\n",
    "            \"n_shared\"   : 2,\n",
    "            \"lambda_sparse\": 0,\n",
    "            \"optimizer_fn\" : Adam,\n",
    "            \"optimizer_params\" : {\"lr\": 4e-2},\n",
    "            \"mask_type\" : \"entmax\",\n",
    "            \"scheduler_params\" : dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "            \"scheduler_fn\" : CosineAnnealingWarmRestarts,\n",
    "            \"seed\" : SEED,\n",
    "            \"verbose\": 10\n",
    "        }\n",
    "\n",
    "    }\n",
    "    bucket_windows = [100, 200, 300, 400, 500]\n",
    "    random_state = SEED\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d557ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:54.876433Z",
     "iopub.status.busy": "2021-09-25T21:00:54.871122Z",
     "iopub.status.idle": "2021-09-25T21:00:54.888820Z",
     "shell.execute_reply": "2021-09-25T21:00:54.889425Z",
     "shell.execute_reply.started": "2021-09-25T20:55:23.097729Z"
    },
    "papermill": {
     "duration": 0.03564,
     "end_time": "2021-09-25T21:00:54.889609",
     "exception": false,
     "start_time": "2021-09-25T21:00:54.853969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# order book features\n",
    "def get_book_features(file_path):\n",
    "    book_df = pd.read_parquet(file_path)\n",
    "\n",
    "    # calculate wap\n",
    "    book_df['wap1'] = calculate_wap(book_df, rank=\"1\")\n",
    "    book_df['wap2'] = calculate_wap(book_df, rank=\"2\")\n",
    "    book_df['iwap1'] = calculate_inter_wap(book_df, rank=\"1\")\n",
    "    book_df['iwap2'] = calculate_inter_wap(book_df, rank=\"2\")\n",
    "\n",
    "    # calculate log return\n",
    "    book_df[\"log_return1\"] = book_df.groupby([\"time_id\"])[\"wap1\"].apply(calculate_log_return)\n",
    "    book_df[\"log_return2\"] = book_df.groupby([\"time_id\"])[\"wap2\"].apply(calculate_log_return)\n",
    "    book_df[\"inter_log_return1\"] = book_df.groupby([\"time_id\"])[\"iwap1\"].apply(calculate_log_return)\n",
    "    book_df[\"inter_log_return2\"] = book_df.groupby([\"time_id\"])[\"iwap2\"].apply(calculate_log_return)\n",
    "\n",
    "    # calculate balance\n",
    "    book_df[\"wap_balance\"] = abs(book_df[\"wap1\"] - book_df[\"wap2\"])\n",
    "    book_df[\"volume_imbalance\"] = abs(\n",
    "        (book_df[\"ask_size1\"] + book_df[\"ask_size2\"]) - (book_df[\"bid_size1\"] + book_df[\"bid_size2\"]))\n",
    "    book_df[\"total_volume\"] = book_df[\"ask_size1\"] + book_df[\"ask_size2\"] + book_df[\"bid_size1\"] + book_df[\n",
    "        \"bid_size2\"]\n",
    "\n",
    "    # calculate spread\n",
    "    book_df[\"price_spread1\"] = (book_df[\"ask_price1\"] - book_df[\"bid_price1\"]) / (\n",
    "            (book_df[\"ask_price1\"] + book_df[\"bid_price1\"]) / 2)\n",
    "    book_df[\"price_spread2\"] = (book_df[\"ask_price2\"] - book_df[\"bid_price2\"]) / (\n",
    "            (book_df[\"ask_price2\"] + book_df[\"bid_price2\"]) / 2)\n",
    "    book_df[\"bid_spread\"] = book_df[\"bid_price1\"] - book_df[\"bid_price2\"]\n",
    "    book_df[\"ask_spread\"] = book_df[\"ask_price1\"] - book_df[\"ask_price2\"]\n",
    "\n",
    "    book_df_merged = window_stats(book_df, cfg.feature_dict_book, cfg.feature_dict_book_time, cfg.bucket_windows)\n",
    "\n",
    "    book_df_merged[\"row_id\"] = book_df_merged[\"time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    book_df_merged.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return book_df_merged.bfill().ffill()\n",
    "                                                                \n",
    "# trade features\n",
    "def get_trade_price_features(df):\n",
    "    res = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]\n",
    "        vol_tendency = tendency(df_id['price'].values, df_id['size'].values)\n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max = np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min = np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))\n",
    "        energy = np.mean(df_id['price'].values ** 2)\n",
    "        iqr_p = np.percentile(df_id['price'].values, 75) - np.percentile(df_id['price'].values, 25)\n",
    "        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))\n",
    "        energy_v = np.sum(df_id['size'].values ** 2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values, 75) - np.percentile(df_id['size'].values, 25)\n",
    "\n",
    "        res.append({'time_id': n_time_id,\n",
    "                    'tendency': vol_tendency,\n",
    "                    'f_max': f_max,\n",
    "                    'f_min': f_min,\n",
    "                    'df_max': df_max,\n",
    "                    'df_min': df_min,\n",
    "                    'abs_diff': abs_diff,\n",
    "                    'energy': energy,\n",
    "                    'iqr_p': iqr_p,\n",
    "                    'abs_diff_v': abs_diff_v,\n",
    "                    'energy_v': energy_v,\n",
    "                    'iqr_p_v': iqr_p_v})\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "    pass\n",
    "\n",
    "\n",
    "def tau_features(df, sec, weight):\n",
    "    tau_feat = 'tau_' + str(sec)\n",
    "    bucket_col = 'trade_seconds_in_bucket_count_unique_' + str(sec)\n",
    "    df[tau_feat] = np.sqrt(weight/df[bucket_col])\n",
    "\n",
    "    size_feat = 'size_' + str(sec)\n",
    "    order_col = 'trade_order_count_sum_' + str(sec)\n",
    "    df[size_feat] = np.sqrt(weight/df[order_col])\n",
    "\n",
    "    return df\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_trade_features(file_path, buck_windows=cfg.bucket_windows):\n",
    "    trade_df = pd.read_parquet(file_path)\n",
    "\n",
    "    trade_df[\"log_return\"] = trade_df.groupby([\"time_id\"])[\"price\"].apply(calculate_log_return)\n",
    "    trade_df[\"amount\"] = trade_df[\"size\"] * trade_df[\"price\"]\n",
    "\n",
    "    price_features = get_trade_price_features(trade_df)\n",
    "    trade_df_merged = window_stats(trade_df, cfg.feature_dict_trade, cfg.feature_dict_trade_time, buck_windows, additional_dfs=price_features)\n",
    "\n",
    "    trade_df_merged = trade_df_merged.add_prefix(\"trade_\")\n",
    "\n",
    "    trade_df_merged[\"row_id\"] = trade_df_merged[\"trade_time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    trade_df_merged.drop([\"trade_time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    for sec in buck_windows:\n",
    "        trade_df_merged = tau_features(trade_df_merged, sec, weight=sec/600)\n",
    "    return trade_df_merged.bfill().ffill()                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0abc836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:54.952612Z",
     "iopub.status.busy": "2021-09-25T21:00:54.943790Z",
     "iopub.status.idle": "2021-09-25T21:00:54.956955Z",
     "shell.execute_reply": "2021-09-25T21:00:54.957682Z",
     "shell.execute_reply.started": "2021-09-25T20:55:23.124041Z"
    },
    "papermill": {
     "duration": 0.048824,
     "end_time": "2021-09-25T21:00:54.957899",
     "exception": false,
     "start_time": "2021-09-25T21:00:54.909075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class GetData:\n",
    "    def __init__(self, df, book_path, trade_path, is_train=True):\n",
    "        self.df = df.copy(deep=True)\n",
    "        self.order_book_path = book_path\n",
    "        self.trade_path = trade_path\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self._get_rowid()\n",
    "\n",
    "    def _get_rowid(self):\n",
    "        self.df[\"row_id\"] = self.df[\"stock_id\"].astype(str) + \"-\" + self.df[\"time_id\"].astype(str)\n",
    "\n",
    "    def get_time_stock(self, buck_windows=cfg.bucket_windows):\n",
    "        vol_cols = []\n",
    "        feat_set = ['log_return1_calculate_rv', 'log_return2_calculate_rv', 'trade_log_return_calculate_rv']\n",
    "        for feat in feat_set:\n",
    "            for sec in buck_windows:\n",
    "                vol_cols.append(feat + f'_{sec}')\n",
    "        vol_cols += feat_set\n",
    "\n",
    "        df_stock_id = self.df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "        df_time_id = self.df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "        df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "        # Merge with original dataframe\n",
    "        self.df = self.df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "        self.df = self.df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "        self.df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def process_features(self, list_stock_ids):\n",
    "        def parallel_helper(stock_id):\n",
    "            book_sample_path = os.path.join(self.order_book_path, f\"stock_id={stock_id}\")\n",
    "            trade_sample_path = os.path.join(self.trade_path, f\"stock_id={stock_id}\")\n",
    "\n",
    "            return pd.merge(get_book_features(book_sample_path), get_trade_features(trade_sample_path),\n",
    "                            on=\"row_id\",\n",
    "                            how=\"left\")\n",
    "\n",
    "        df = Parallel(n_jobs=-1, verbose=1)(delayed(parallel_helper)(stock_id) for stock_id in list_stock_ids)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _get_features(self):\n",
    "        features_df = self.process_features(self.df[\"stock_id\"].unique())\n",
    "        self.df = self.df.merge(features_df, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "        return self.get_time_stock()\n",
    "        pass\n",
    "\n",
    "    def get_all_features(self, stock_groups):\n",
    "        return create_cluster_aggregations(self._get_features(), stock_groups)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f766b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:55.013706Z",
     "iopub.status.busy": "2021-09-25T21:00:55.012853Z",
     "iopub.status.idle": "2021-09-25T21:00:55.030372Z",
     "shell.execute_reply": "2021-09-25T21:00:55.031058Z",
     "shell.execute_reply.started": "2021-09-25T20:55:23.141829Z"
    },
    "papermill": {
     "duration": 0.057479,
     "end_time": "2021-09-25T21:00:55.031275",
     "exception": false,
     "start_time": "2021-09-25T21:00:54.973796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred)/y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score)/y_true)))\n",
    "    \n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean(((y_true - y_pred) / y_true)**2)).clone()\n",
    "\n",
    "def get_transform(df, name):\n",
    "    label_stocks = LabelEncoder()\n",
    "    df['stock_id'] = label_stocks.fit_transform(df['stock_id'])\n",
    "    pickle.dump(label_stocks, open(\"./label_stocks.pkl\", \"wb\"))\n",
    "    \n",
    "    print(f\"[INFO] Using {name} scaler...\\n\")\n",
    "    if name==\"mm\":\n",
    "        scaler = MinMaxScaler().fit(df.drop([\"stock_id\"], axis=1))\n",
    "    elif name==\"mm_11\":\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1)).fit(df.drop([\"stock_id\"], axis=1))\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(df.drop([\"stock_id\"], axis=1))\n",
    "        \n",
    "    df.iloc[:, 1:] = scaler.transform(df.iloc[:, 1:])\n",
    "    pickle.dump(scaler, open(f\"./{name}.pkl\", \"wb\"))\n",
    "    \n",
    "    return df\n",
    "    pass\n",
    "\n",
    "class TrainFer:\n",
    "    def __init__(self, params_dict, n_splits, model_path, random_state):\n",
    "        self.params = params_dict\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.model_path = model_path\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "            \n",
    "    \n",
    "    def train(self, X, y, scaler_name=\"mm\"):\n",
    "        X = get_transform(X, scaler_name)\n",
    "        \n",
    "        oof_predictions = np.zeros(X.shape[0])\n",
    "        kfold = KFold(n_splits=self.n_splits, random_state=0, shuffle=True)\n",
    "        oof_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "            print(f\"\\nFold - {fold}\\n\")\n",
    "\n",
    "            x_train, y_train = X.iloc[train_idx].values, y.iloc[train_idx].values.reshape(-1, 1)\n",
    "            x_val, y_val = X.iloc[val_idx].values, y.iloc[val_idx].values.reshape(-1, 1)\n",
    "\n",
    "            model = TabNetRegressor(**cfg.model_params[\"tabnet\"])\n",
    "            model.fit(x_train, y_train,\n",
    "                      eval_set=[(x_val, y_val)],\n",
    "                      max_epochs=200,\n",
    "                      patience=50,\n",
    "                      batch_size=1024*20, \n",
    "                      virtual_batch_size=128*20,\n",
    "                      drop_last=False,\n",
    "                      eval_metric=[RMSPE],\n",
    "                      loss_fn=RMSPELoss\n",
    "              )\n",
    "\n",
    "            fold_preds = model.predict(x_val)\n",
    "            oof_score = rmspe(y_val, fold_preds)\n",
    "            print(f\"\\nRMSPE of fold {fold}: {oof_score}\")\n",
    "            model.save_model(os.path.join(self.model_path, f\"lgb_bl_{fold}_{oof_score}.pkl\"))\n",
    "            \n",
    "            oof_scores.append(oof_score)\n",
    "            oof_predictions[val_idx] = fold_preds\n",
    "        \n",
    "        print(f\"\\nOOF Scores: {oof_scores}\\n\")\n",
    "        rmspe_score = rmspe(y, oof_predictions)\n",
    "        print(f\"OOF RMSPE: {rmspe_score}\")\n",
    "        \n",
    "        return y, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76a2d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T21:00:55.192995Z",
     "iopub.status.busy": "2021-09-25T21:00:55.192088Z",
     "iopub.status.idle": "2021-09-25T21:35:30.642332Z",
     "shell.execute_reply": "2021-09-25T21:35:30.640490Z"
    },
    "papermill": {
     "duration": 2075.595508,
     "end_time": "2021-09-25T21:35:30.642604",
     "exception": true,
     "start_time": "2021-09-25T21:00:55.047096",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using ss scaler...\n",
      "\n",
      "\n",
      "Fold - 0\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 139.7404| val_0_rmspe: 27.38322|  0:00:11s\n",
      "epoch 10 | loss: 0.52564 | val_0_rmspe: 0.39378 |  0:01:53s\n",
      "epoch 20 | loss: 0.4044  | val_0_rmspe: 0.45489 |  0:03:34s\n",
      "epoch 30 | loss: 0.37514 | val_0_rmspe: 0.29373 |  0:05:15s\n",
      "epoch 40 | loss: 0.2865  | val_0_rmspe: 0.26651 |  0:06:56s\n",
      "epoch 50 | loss: 0.27227 | val_0_rmspe: 0.26827 |  0:08:37s\n",
      "epoch 60 | loss: 0.26974 | val_0_rmspe: 0.26153 |  0:10:17s\n",
      "epoch 70 | loss: 0.26342 | val_0_rmspe: 0.25897 |  0:11:57s\n",
      "epoch 80 | loss: 0.26331 | val_0_rmspe: 0.25984 |  0:13:36s\n",
      "epoch 90 | loss: 0.25434 | val_0_rmspe: 0.25175 |  0:15:16s\n",
      "epoch 100| loss: 0.25175 | val_0_rmspe: 0.24956 |  0:16:57s\n",
      "epoch 110| loss: 0.24529 | val_0_rmspe: 0.24319 |  0:18:37s\n",
      "epoch 120| loss: 0.23653 | val_0_rmspe: 0.23566 |  0:20:18s\n",
      "epoch 130| loss: 0.23058 | val_0_rmspe: 0.23086 |  0:21:58s\n",
      "epoch 140| loss: 0.23007 | val_0_rmspe: 0.23098 |  0:23:39s\n",
      "epoch 150| loss: 0.22698 | val_0_rmspe: 0.22821 |  0:25:18s\n",
      "epoch 160| loss: 0.22555 | val_0_rmspe: 0.22625 |  0:26:58s\n",
      "epoch 170| loss: 0.22499 | val_0_rmspe: 0.22489 |  0:28:39s\n",
      "epoch 180| loss: 0.22401 | val_0_rmspe: 0.22462 |  0:30:19s\n",
      "epoch 190| loss: 0.22358 | val_0_rmspe: 0.22454 |  0:31:59s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 195 and best_val_0_rmspe = 0.22422\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "RMSPE of fold 0: 0.22422221659023772\n",
      "Successfully saved model at ./tabnet/lgb_bl_0_0.22422221659023772.pkl.zip\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (85787,1) could not be broadcast to indexing result of shape (85787,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bbfb59785415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainFer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tabnet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tabnet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"row_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"time_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b2557e374de9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, scaler_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0moof_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0moof_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nOOF Scores: {oof_scores}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (85787,1) could not be broadcast to indexing result of shape (85787,)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    train_feats = pickle.load(open(\"../input/processed-dataset-orvp/train_df.pkl\", \"rb\"))\n",
    "    train_feats.fillna(-1, inplace=True)\n",
    "    reg = TrainFer(cfg.model_params[\"tabnet\"], n_splits=5, model_path=cfg.paths[\"tabnet\"], random_state=cfg.random_state) \n",
    "    y_targets, oof_preds_ss = reg.train(train_feats.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_feats[\"target\"], \"ss\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706aa14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2113.17738,
   "end_time": "2021-09-25T21:35:31.962861",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-25T21:00:18.785481",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
