{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "shaped-olympus",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:27.930314Z",
     "iopub.status.busy": "2021-08-22T08:03:27.929055Z",
     "iopub.status.idle": "2021-08-22T08:03:29.056127Z",
     "shell.execute_reply": "2021-08-22T08:03:29.056613Z",
     "shell.execute_reply.started": "2021-08-22T08:02:37.944355Z"
    },
    "papermill": {
     "duration": 1.14444,
     "end_time": "2021-08-22T08:03:29.056942",
     "exception": false,
     "start_time": "2021-08-22T08:03:27.912502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complicated-controversy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.087003Z",
     "iopub.status.busy": "2021-08-22T08:03:29.086063Z",
     "iopub.status.idle": "2021-08-22T08:03:29.388860Z",
     "shell.execute_reply": "2021-08-22T08:03:29.388232Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.116495Z"
    },
    "papermill": {
     "duration": 0.320073,
     "end_time": "2021-08-22T08:03:29.389003",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.068930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Info: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 428932 entries, 0 to 428931\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   stock_id  428932 non-null  int64  \n",
      " 1   time_id   428932 non-null  int64  \n",
      " 2   target    428932 non-null  float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 9.8 MB\n",
      "\n",
      "Test Info: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   stock_id  3 non-null      int64 \n",
      " 1   time_id   3 non-null      int64 \n",
      " 2   row_id    3 non-null      object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 200.0+ bytes\n",
      "\n",
      "Sample Submission Format: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   row_id  3 non-null      object \n",
      " 1   target  3 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 176.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# files\n",
    "train = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "test = pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "ss = pd.read_csv(\"../input/optiver-realized-volatility-prediction/sample_submission.csv\")\n",
    "\n",
    "print(\"Train Info: \\n\")\n",
    "train.info()\n",
    "\n",
    "print(\"\\nTest Info: \\n\")\n",
    "test.info()\n",
    "\n",
    "print(\"\\nSample Submission Format: \\n\")\n",
    "ss.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-lying",
   "metadata": {
    "papermill": {
     "duration": 0.01175,
     "end_time": "2021-08-22T08:03:29.413208",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.401458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-glenn",
   "metadata": {
    "papermill": {
     "duration": 0.01165,
     "end_time": "2021-08-22T08:03:29.436948",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.425298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Alright, we got a basic idea of what and how of the problem statement. Let's get on to it. There are actually pretty good baselines, so instead of describing more feature-centric approaches, I will build a XGB pipeline here. Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upset-connectivity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.471743Z",
     "iopub.status.busy": "2021-08-22T08:03:29.470729Z",
     "iopub.status.idle": "2021-08-22T08:03:29.474075Z",
     "shell.execute_reply": "2021-08-22T08:03:29.473434Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.479085Z"
    },
    "papermill": {
     "duration": 0.025262,
     "end_time": "2021-08-22T08:03:29.474217",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.448955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature utils\n",
    "def calculate_wap(df, rank=\"1\"):\n",
    "    \"\"\"\n",
    "    Weighted Average Pricing for a stock at a given time ID is given by:\n",
    "    (bid_price1 * ask_size1 + bid_size1 * ask_price1)/(bid_size1 + ask_size1)\n",
    "\n",
    "    It can further be extended to:\n",
    "\n",
    "        sum(bid_price_i * ask_size_i + bid_size_i * ask_price_i)/sum(bid_size_i + ask_size_i)\n",
    "\n",
    "    :param rank: which wap to calculate\n",
    "    :param df: parquet table containing order book\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"bid_size{rank}\"] * df[f\"ask_price{rank}\"]) / (df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n",
    "\n",
    "\n",
    "def calculate_logreturn(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def calculate_rv(series):\n",
    "    return np.sqrt(np.sum(np.square(series)))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def get_stats_window(df, seconds_in_bucket, features_dict, add_suffix=False):\n",
    "    df_feature = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(features_dict).reset_index()\n",
    "    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n",
    "    \n",
    "    if add_suffix:\n",
    "        df_feature = df_feature.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        \n",
    "    return df_feature\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immune-traveler",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.501868Z",
     "iopub.status.busy": "2021-08-22T08:03:29.501100Z",
     "iopub.status.idle": "2021-08-22T08:03:29.511620Z",
     "shell.execute_reply": "2021-08-22T08:03:29.512178Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.492359Z"
    },
    "papermill": {
     "duration": 0.025963,
     "end_time": "2021-08-22T08:03:29.512353",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.486390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "class cfg:\n",
    "    \n",
    "    paths = {\n",
    "        # train path\n",
    "        \"train_csv\": \"../input/optiver-realized-volatility-prediction/train.csv\",\n",
    "        \"train_book\": \"../input/optiver-realized-volatility-prediction/book_train.parquet\",\n",
    "        \"train_trade\": \"../input/optiver-realized-volatility-prediction/trade_train.parquet\",\n",
    "\n",
    "        # test path\n",
    "        \"test_csv\": \"../input/optiver-realized-volatility-prediction/test.csv\",\n",
    "        \"test_book\": \"../input/optiver-realized-volatility-prediction/book_test.parquet\",\n",
    "        \"test_trade\": \"../input/optiver-realized-volatility-prediction/trade_test.parquet\",\n",
    "    }\n",
    "\n",
    "    feature_dict_book = {\n",
    "        \"wap1\": [np.sum, np.mean, np.std],\n",
    "        \"wap2\": [np.sum, np.mean, np.std],\n",
    "        \"log_return1\": [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"log_return2\": [np.sum, calculate_rv, np.mean, np.std],\n",
    "        \"wap_balance\": [np.sum, np.mean, np.std],\n",
    "        \"volume_imbalance\": [np.sum, np.mean, np.std],\n",
    "        \"total_volume\": [np.sum, np.mean, np.std],\n",
    "        \"price_spread1\": [np.sum, np.mean, np.std],\n",
    "        \"price_spread2\": [np.sum, np.mean, np.std],\n",
    "        \"bid_spread\": [np.sum, np.mean, np.std],\n",
    "        \"ask_spread\": [np.sum, np.mean, np.std],\n",
    "    }\n",
    "\n",
    "    feature_dict_trade = {\n",
    "        \"log_return\": [calculate_rv],\n",
    "        \"seconds_in_bucket\": [count_unique],\n",
    "        \"size\": [np.sum],\n",
    "        \"order_count\": [np.mean]\n",
    "    }\n",
    "    \n",
    "    model_params = {\n",
    "        \"xgb_bl\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"nthread\": -1,\n",
    "            \"eta\": 0.3,\n",
    "            \"max_depth\": 8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"sampling_method\": \"gradient_based\",\n",
    "#             \"tree_method\": \"gpu_hist\"  # turn it on for GPU\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "green-spider",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.540658Z",
     "iopub.status.busy": "2021-08-22T08:03:29.539871Z",
     "iopub.status.idle": "2021-08-22T08:03:29.559952Z",
     "shell.execute_reply": "2021-08-22T08:03:29.560456Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.508213Z"
    },
    "papermill": {
     "duration": 0.036072,
     "end_time": "2021-08-22T08:03:29.560656",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.524584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# order book features\n",
    "def get_book_features(file_path):\n",
    "    book_df = pd.read_parquet(file_path)\n",
    "\n",
    "    # calculate wap\n",
    "    book_df['wap1'] = calculate_wap(book_df, rank=\"1\")\n",
    "    book_df['wap2'] = calculate_wap(book_df, rank=\"2\")\n",
    "\n",
    "    # calculate log return\n",
    "    book_df[\"log_return1\"] = book_df.groupby([\"time_id\"])[\"wap1\"].apply(calculate_logreturn)\n",
    "    book_df[\"log_return2\"] = book_df.groupby([\"time_id\"])[\"wap2\"].apply(calculate_logreturn)\n",
    "\n",
    "    # calculate balance\n",
    "    book_df[\"wap_balance\"] = abs(book_df[\"wap1\"] - book_df[\"wap2\"])\n",
    "    book_df[\"volume_imbalance\"] = abs(\n",
    "        (book_df[\"ask_size1\"] + book_df[\"ask_size2\"]) - (book_df[\"bid_size1\"] + book_df[\"bid_size2\"]))\n",
    "    book_df[\"total_volume\"] = book_df[\"ask_size1\"] + book_df[\"ask_size2\"] + book_df[\"bid_size1\"] + book_df[\n",
    "        \"bid_size2\"]\n",
    "\n",
    "    # calculate spread\n",
    "    book_df[\"price_spread1\"] = (book_df[\"ask_price1\"] - book_df[\"bid_price1\"]) / (\n",
    "            (book_df[\"ask_price1\"] + book_df[\"bid_price1\"]) / 2)\n",
    "    book_df[\"price_spread2\"] = (book_df[\"ask_price2\"] - book_df[\"bid_price2\"]) / (\n",
    "            (book_df[\"ask_price2\"] + book_df[\"bid_price2\"]) / 2)\n",
    "\n",
    "    book_df[\"bid_spread\"] = book_df[\"bid_price1\"] - book_df[\"bid_price2\"]\n",
    "    book_df[\"ask_spread\"] = book_df[\"ask_price1\"] - book_df[\"ask_price2\"]\n",
    "\n",
    "    book_df_merged = get_stats_window(book_df, seconds_in_bucket=0, features_dict=cfg.feature_dict_book)\n",
    "\n",
    "    book_df_450 = get_stats_window(book_df, seconds_in_bucket=450, features_dict=cfg.feature_dict_book, add_suffix=True)\n",
    "    book_df_300 = get_stats_window(book_df, seconds_in_bucket=300, features_dict=cfg.feature_dict_book, add_suffix=True)\n",
    "    book_df_150 = get_stats_window(book_df, seconds_in_bucket=150, features_dict=cfg.feature_dict_book, add_suffix=True)\n",
    "\n",
    "    # merge stats\n",
    "    book_df_merged = book_df_merged.merge(book_df_450, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__450\")\n",
    "    book_df_merged = book_df_merged.merge(book_df_300, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__300\")\n",
    "    book_df_merged = book_df_merged.merge(book_df_150, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__150\")\n",
    "\n",
    "\n",
    "    book_df_merged.drop(columns=[\"time_id__450\", \"time_id__300\", \"time_id__150\"], inplace=True)\n",
    "\n",
    "    book_df_merged[\"row_id\"] = book_df_merged[\"time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    book_df_merged.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return book_df_merged\n",
    "\n",
    "# trade features\n",
    "def get_trade_features(file_path):\n",
    "    trade_df = pd.read_parquet(file_path)\n",
    "    \n",
    "    trade_df[\"log_return\"] = trade_df.groupby([\"time_id\"])[\"price\"].apply(calculate_logreturn)\n",
    "\n",
    "    trade_df_merged = get_stats_window(trade_df, seconds_in_bucket=0, features_dict=cfg.feature_dict_trade)\n",
    "\n",
    "    trade_df_450 = get_stats_window(trade_df, seconds_in_bucket=450, features_dict=cfg.feature_dict_trade, add_suffix=True)\n",
    "    trade_df_300 = get_stats_window(trade_df, seconds_in_bucket=300, features_dict=cfg.feature_dict_trade, add_suffix=True)\n",
    "    trade_df_150 = get_stats_window(trade_df, seconds_in_bucket=150, features_dict=cfg.feature_dict_trade, add_suffix=True)\n",
    "\n",
    "    # merge stats\n",
    "    trade_df_merged = trade_df_merged.merge(trade_df_450, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__450\")\n",
    "    trade_df_merged = trade_df_merged.merge(trade_df_300, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__300\")\n",
    "    trade_df_merged = trade_df_merged.merge(trade_df_150, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__150\")\n",
    "    \n",
    "    trade_df_merged.drop(columns=[\"time_id__450\", \"time_id__300\", \"time_id__150\"], inplace=True)\n",
    "    \n",
    "    trade_df_merged = trade_df_merged.add_prefix(\"trade_\")\n",
    "\n",
    "    trade_df_merged[\"row_id\"] = trade_df_merged[\"trade_time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n",
    "    trade_df_merged.drop([\"trade_time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return trade_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inner-fitting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.588432Z",
     "iopub.status.busy": "2021-08-22T08:03:29.587728Z",
     "iopub.status.idle": "2021-08-22T08:03:29.602782Z",
     "shell.execute_reply": "2021-08-22T08:03:29.603279Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.533780Z"
    },
    "papermill": {
     "duration": 0.030535,
     "end_time": "2021-08-22T08:03:29.603465",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.572930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetData:\n",
    "    def __init__(self, df, book_path, trade_path):\n",
    "        self.df = df.copy(deep=True)\n",
    "        self.order_book_path = book_path\n",
    "        self.trade_path = trade_path\n",
    "\n",
    "        self._get_rowid()\n",
    "\n",
    "    def _get_rowid(self):\n",
    "        self.df[\"row_id\"] = self.df[\"stock_id\"].astype(str) + \"-\" + self.df[\"time_id\"].astype(str)\n",
    "\n",
    "    def get_time_stock(self):\n",
    "        vol_cols = ['log_return1_calculate_rv', 'log_return2_calculate_rv',\n",
    "                    'log_return1_calculate_rv_450', 'log_return2_calculate_rv_450',\n",
    "                    'log_return1_calculate_rv_300', 'log_return2_calculate_rv_300',\n",
    "                    'log_return1_calculate_rv_150', 'log_return2_calculate_rv_150',\n",
    "                    'trade_log_return_calculate_rv', 'trade_log_return_calculate_rv_450',\n",
    "                    'trade_log_return_calculate_rv_300', 'trade_log_return_calculate_rv_150']\n",
    "\n",
    "        df_stock_id = self.df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "        df_time_id = self.df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "        df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "        # Merge with original dataframe\n",
    "        self.df = self.df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "        self.df = self.df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "        self.df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def process_features(self, list_stock_ids):\n",
    "        def parallel_helper(stock_id):\n",
    "            book_sample_path = os.path.join(self.order_book_path, f\"stock_id={stock_id}\")\n",
    "            trade_sample_path = os.path.join(self.trade_path, f\"stock_id={stock_id}\")\n",
    "\n",
    "            return pd.merge(get_book_features(book_sample_path), get_trade_features(trade_sample_path),\n",
    "                            on=\"row_id\",\n",
    "                            how=\"left\")\n",
    "\n",
    "        df = Parallel(n_jobs=-1, verbose=1)(delayed(parallel_helper)(stock_id) for stock_id in list_stock_ids)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_features(self):\n",
    "        features_df = self.process_features(self.df[\"stock_id\"].unique())\n",
    "        self.df = self.df.merge(features_df, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "        return self.get_time_stock()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-temple",
   "metadata": {
    "papermill": {
     "duration": 0.011791,
     "end_time": "2021-08-22T08:03:29.627482",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.615691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "final-aberdeen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.654291Z",
     "iopub.status.busy": "2021-08-22T08:03:29.653670Z",
     "iopub.status.idle": "2021-08-22T08:03:29.668949Z",
     "shell.execute_reply": "2021-08-22T08:03:29.669466Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.554271Z"
    },
    "papermill": {
     "duration": 0.030217,
     "end_time": "2021-08-22T08:03:29.669649",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.639432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, xgb_dtrain):\n",
    "    y_true = xgb_dtrain.get_label()\n",
    "    return \"RMSPE\", rmspe(y_true, y_pred)\n",
    "\n",
    "\n",
    "class TrainFer:\n",
    "    def __init__(self, params_dict, n_splits, model_path, random_state=2021):\n",
    "        self.params = params_dict\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        oof_predictions = np.zeros(X.shape[0])\n",
    "        kfold = KFold(n_splits=self.n_splits, random_state=self.random_state, shuffle=True)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "            print(f\"\\nFold - {fold}\\n\")\n",
    "\n",
    "            x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "            \n",
    "            x_train[\"stock_id\"] = x_train[\"stock_id\"].astype(int)\n",
    "            x_val[\"stock_id\"] = x_val[\"stock_id\"].astype(int)\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, weight=1/np.square(y_train), enable_categorical=True)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val, weight=1/np.square(y_val), enable_categorical=True)\n",
    "\n",
    "            model = xgb.train(self.params,\n",
    "                              dtrain=dtrain,\n",
    "                              num_boost_round=100,\n",
    "                              evals=[(dtrain, \"dtrain\"), (dval, \"dval\")],\n",
    "                              verbose_eval=10,\n",
    "                              feval=feval_rmspe)\n",
    "            \n",
    "            pickle.dump(model, open(os.path.join(self.model_path, f\"xgb_bl_{fold}.pkl\"), \"wb\"))\n",
    "            oof_predictions[val_idx] = model.predict(dval)\n",
    "            \n",
    "        rmspe_score = rmspe(y, oof_predictions)\n",
    "        print(f\"OOF RMSPE: {rmspe_score}\")\n",
    "        \n",
    "    def infer(self, x_test):\n",
    "        test_predictions = np.zeros(x_test.shape[0])\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "        \n",
    "        for mpth in os.listdir(self.model_path):\n",
    "            model = pickle.load(open(os.path.join(self.model_path, mpth), \"rb\"))\n",
    "            test_predictions += model.predict(dtest)/5\n",
    "            \n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stylish-civilian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:29.697654Z",
     "iopub.status.busy": "2021-08-22T08:03:29.696996Z",
     "iopub.status.idle": "2021-08-22T08:03:31.052170Z",
     "shell.execute_reply": "2021-08-22T08:03:31.052660Z",
     "shell.execute_reply.started": "2021-08-22T08:02:39.576685Z"
    },
    "papermill": {
     "duration": 1.371114,
     "end_time": "2021-08-22T08:03:31.052868",
     "exception": false,
     "start_time": "2021-08-22T08:03:29.681754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>log_return1_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_log_return_calculate_rv_450_max_time</th>\n",
       "      <th>trade_log_return_calculate_rv_450_min_time</th>\n",
       "      <th>trade_log_return_calculate_rv_300_mean_time</th>\n",
       "      <th>trade_log_return_calculate_rv_300_std_time</th>\n",
       "      <th>trade_log_return_calculate_rv_300_max_time</th>\n",
       "      <th>trade_log_return_calculate_rv_300_min_time</th>\n",
       "      <th>trade_log_return_calculate_rv_150_mean_time</th>\n",
       "      <th>trade_log_return_calculate_rv_150_std_time</th>\n",
       "      <th>trade_log_return_calculate_rv_150_max_time</th>\n",
       "      <th>trade_log_return_calculate_rv_150_min_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>1.000405</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>3.00165</td>\n",
       "      <td>1.00055</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  wap1_sum  wap1_mean  wap1_std  wap2_sum  \\\n",
       "0         0        4    0-4  3.001215   1.000405   0.00017   3.00165   \n",
       "1         0       32   0-32       NaN        NaN       NaN       NaN   \n",
       "2         0       34   0-34       NaN        NaN       NaN       NaN   \n",
       "\n",
       "   wap2_mean  wap2_std  log_return1_sum  ...  \\\n",
       "0    1.00055  0.000153         0.000294  ...   \n",
       "1        NaN       NaN              NaN  ...   \n",
       "2        NaN       NaN              NaN  ...   \n",
       "\n",
       "   trade_log_return_calculate_rv_450_max_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_450_min_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_300_mean_time  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_300_std_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_300_max_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_300_min_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_150_mean_time  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_150_std_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_150_max_time  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   trade_log_return_calculate_rv_150_min_time  \n",
       "0                                         NaN  \n",
       "1                                         NaN  \n",
       "2                                         NaN  \n",
       "\n",
       "[3 rows x 255 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = GetData(test, cfg.paths[\"test_book\"], cfg.paths[\"test_trade\"])\n",
    "test_df = test_data.get_features()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "patient-conversion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:31.094955Z",
     "iopub.status.busy": "2021-08-22T08:03:31.094083Z",
     "iopub.status.idle": "2021-08-22T08:03:32.900258Z",
     "shell.execute_reply": "2021-08-22T08:03:32.900867Z",
     "shell.execute_reply.started": "2021-08-22T08:02:41.037719Z"
    },
    "papermill": {
     "duration": 1.834258,
     "end_time": "2021-08-22T08:03:32.901047",
     "exception": false,
     "start_time": "2021-08-22T08:03:31.066789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tester = TrainFer(cfg.model_params[\"xgb_bl\"], n_splits=5, model_path=\"../input/orvpxgbbaselinelocalv1/\")\n",
    "preds = tester.infer(test_df.drop(columns=[\"row_id\", \"time_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pediatric-southwest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T08:03:32.935227Z",
     "iopub.status.busy": "2021-08-22T08:03:32.934561Z",
     "iopub.status.idle": "2021-08-22T08:03:32.941987Z",
     "shell.execute_reply": "2021-08-22T08:03:32.941443Z",
     "shell.execute_reply.started": "2021-08-22T08:02:43.276359Z"
    },
    "papermill": {
     "duration": 0.027678,
     "end_time": "2021-08-22T08:03:32.942143",
     "exception": false,
     "start_time": "2021-08-22T08:03:32.914465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[\"target\"] = preds\n",
    "test[[\"row_id\", \"target\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-appendix",
   "metadata": {
    "papermill": {
     "duration": 0.013115,
     "end_time": "2021-08-22T08:03:32.969040",
     "exception": false,
     "start_time": "2021-08-22T08:03:32.955925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Work In Progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.860474,
   "end_time": "2021-08-22T08:03:34.509937",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-22T08:03:19.649463",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
