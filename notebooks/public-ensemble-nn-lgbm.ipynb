{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874fa8c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:41.516964Z",
     "iopub.status.busy": "2021-09-18T11:48:41.515722Z",
     "iopub.status.idle": "2021-09-18T11:48:44.121611Z",
     "shell.execute_reply": "2021-09-18T11:48:44.120758Z",
     "shell.execute_reply.started": "2021-09-18T11:29:34.917542Z"
    },
    "papermill": {
     "duration": 2.644243,
     "end_time": "2021-09-18T11:48:44.121776",
     "exception": false,
     "start_time": "2021-09-18T11:48:41.477533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159c39b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:44.213401Z",
     "iopub.status.busy": "2021-09-18T11:48:44.174817Z",
     "iopub.status.idle": "2021-09-18T11:48:44.234268Z",
     "shell.execute_reply": "2021-09-18T11:48:44.234749Z",
     "shell.execute_reply.started": "2021-09-18T11:29:37.566502Z"
    },
    "papermill": {
     "duration": 0.090971,
     "end_time": "2021-09-18T11:48:44.234948",
     "exception": false,
     "start_time": "2021-09-18T11:48:44.143977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_train_test():\n",
    "    #train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    #print(f'Our training set has {train.shape[0]} rows')\n",
    "    return test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c17b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:44.284768Z",
     "iopub.status.busy": "2021-09-18T11:48:44.283871Z",
     "iopub.status.idle": "2021-09-18T11:48:49.712681Z",
     "shell.execute_reply": "2021-09-18T11:48:49.713682Z",
     "shell.execute_reply.started": "2021-09-18T11:29:37.635435Z"
    },
    "papermill": {
     "duration": 5.457635,
     "end_time": "2021-09-18T11:48:49.713866",
     "exception": false,
     "start_time": "2021-09-18T11:48:44.256231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train = pd.read_pickle(\"../input/optiver006/train.pkl\")\n",
    "test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "#train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "#train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "#train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "#train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "train1 = train.copy()\n",
    "test1 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ce0de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:49.767913Z",
     "iopub.status.busy": "2021-09-18T11:48:49.766791Z",
     "iopub.status.idle": "2021-09-18T11:48:49.796527Z",
     "shell.execute_reply": "2021-09-18T11:48:49.795837Z",
     "shell.execute_reply.started": "2021-09-18T11:29:44.526280Z"
    },
    "papermill": {
     "duration": 0.059896,
     "end_time": "2021-09-18T11:48:49.796683",
     "exception": false,
     "start_time": "2021-09-18T11:48:49.736787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aef785a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:49.852459Z",
     "iopub.status.busy": "2021-09-18T11:48:49.851216Z",
     "iopub.status.idle": "2021-09-18T11:48:49.883352Z",
     "shell.execute_reply": "2021-09-18T11:48:49.882753Z",
     "shell.execute_reply.started": "2021-09-18T11:29:44.558933Z"
    },
    "papermill": {
     "duration": 0.062447,
     "end_time": "2021-09-18T11:48:49.883511",
     "exception": false,
     "start_time": "2021-09-18T11:48:49.821064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1644e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:49.934852Z",
     "iopub.status.busy": "2021-09-18T11:48:49.934230Z",
     "iopub.status.idle": "2021-09-18T11:48:49.939180Z",
     "shell.execute_reply": "2021-09-18T11:48:49.938538Z",
     "shell.execute_reply.started": "2021-09-18T11:29:44.595401Z"
    },
    "papermill": {
     "duration": 0.032157,
     "end_time": "2021-09-18T11:48:49.939323",
     "exception": false,
     "start_time": "2021-09-18T11:48:49.907166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28ff9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:49.997302Z",
     "iopub.status.busy": "2021-09-18T11:48:49.996597Z",
     "iopub.status.idle": "2021-09-18T11:48:52.308930Z",
     "shell.execute_reply": "2021-09-18T11:48:52.308292Z",
     "shell.execute_reply.started": "2021-09-18T11:29:44.601894Z"
    },
    "papermill": {
     "duration": 2.347717,
     "end_time": "2021-09-18T11:48:52.309073",
     "exception": false,
     "start_time": "2021-09-18T11:48:49.961356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    }
   ],
   "source": [
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append(newDf)\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append(newDf)\n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2, mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b14274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:48:52.365019Z",
     "iopub.status.busy": "2021-09-18T11:48:52.364301Z",
     "iopub.status.idle": "2021-09-18T11:49:00.706936Z",
     "shell.execute_reply": "2021-09-18T11:49:00.707500Z",
     "shell.execute_reply.started": "2021-09-18T11:29:46.971066Z"
    },
    "papermill": {
     "duration": 8.375508,
     "end_time": "2021-09-18T11:49:00.707680",
     "exception": false,
     "start_time": "2021-09-18T11:48:52.332172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train, mat1[nnn], how='left', on='time_id')\n",
    "test = pd.merge(test, mat2[nnn], how='left', on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a878bf32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:49:00.755568Z",
     "iopub.status.busy": "2021-09-18T11:49:00.754912Z",
     "iopub.status.idle": "2021-09-18T11:49:00.897156Z",
     "shell.execute_reply": "2021-09-18T11:49:00.897639Z",
     "shell.execute_reply.started": "2021-09-18T11:29:55.392898Z"
    },
    "papermill": {
     "duration": 0.167886,
     "end_time": "2021-09-18T11:49:00.897823",
     "exception": false,
     "start_time": "2021-09-18T11:49:00.729937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f35a8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:49:00.946946Z",
     "iopub.status.busy": "2021-09-18T11:49:00.946277Z",
     "iopub.status.idle": "2021-09-18T11:49:00.954467Z",
     "shell.execute_reply": "2021-09-18T11:49:00.954938Z",
     "shell.execute_reply.started": "2021-09-18T11:29:55.534949Z"
    },
    "papermill": {
     "duration": 0.034102,
     "end_time": "2021-09-18T11:49:00.955116",
     "exception": false,
     "start_time": "2021-09-18T11:49:00.921014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed0 = 69\n",
    "seed1 = 2021\n",
    "\n",
    "params69 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    \"learning_rate\": 0.05,\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1}\n",
    "\n",
    "params2021 = {\n",
    "        'learning_rate': 0.05,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54070cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T11:49:01.003902Z",
     "iopub.status.busy": "2021-09-18T11:49:01.003230Z",
     "iopub.status.idle": "2021-09-18T13:08:49.037947Z",
     "shell.execute_reply": "2021-09-18T13:08:49.037040Z"
    },
    "papermill": {
     "duration": 4788.060502,
     "end_time": "2021-09-18T13:08:49.038303",
     "exception": false,
     "start_time": "2021-09-18T11:49:00.977801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Seed: 69\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000429158\ttraining's RMSPE: 0.198556\tvalid_1's rmse: 0.000439344\tvalid_1's RMSPE: 0.203594\n",
      "[500]\ttraining's rmse: 0.000407219\ttraining's RMSPE: 0.188406\tvalid_1's rmse: 0.000425241\tvalid_1's RMSPE: 0.197059\n",
      "[750]\ttraining's rmse: 0.000393828\ttraining's RMSPE: 0.18221\tvalid_1's rmse: 0.000417885\tvalid_1's RMSPE: 0.19365\n",
      "[1000]\ttraining's rmse: 0.000383671\ttraining's RMSPE: 0.177511\tvalid_1's rmse: 0.000413483\tvalid_1's RMSPE: 0.19161\n",
      "[1250]\ttraining's rmse: 0.000375474\ttraining's RMSPE: 0.173719\tvalid_1's rmse: 0.000410394\tvalid_1's RMSPE: 0.190178\n",
      "[1500]\ttraining's rmse: 0.000368734\ttraining's RMSPE: 0.1706\tvalid_1's rmse: 0.000408262\tvalid_1's RMSPE: 0.18919\n",
      "[1750]\ttraining's rmse: 0.00036261\ttraining's RMSPE: 0.167767\tvalid_1's rmse: 0.000406823\tvalid_1's RMSPE: 0.188524\n",
      "[2000]\ttraining's rmse: 0.000357264\ttraining's RMSPE: 0.165293\tvalid_1's rmse: 0.000405488\tvalid_1's RMSPE: 0.187905\n",
      "[2250]\ttraining's rmse: 0.000352265\ttraining's RMSPE: 0.162981\tvalid_1's rmse: 0.00040479\tvalid_1's RMSPE: 0.187582\n",
      "[2500]\ttraining's rmse: 0.000347765\ttraining's RMSPE: 0.160899\tvalid_1's rmse: 0.00040429\tvalid_1's RMSPE: 0.18735\n",
      "[2750]\ttraining's rmse: 0.000343516\ttraining's RMSPE: 0.158933\tvalid_1's rmse: 0.000403919\tvalid_1's RMSPE: 0.187178\n",
      "[3000]\ttraining's rmse: 0.00033957\ttraining's RMSPE: 0.157107\tvalid_1's rmse: 0.000403664\tvalid_1's RMSPE: 0.18706\n",
      "[3250]\ttraining's rmse: 0.00033582\ttraining's RMSPE: 0.155372\tvalid_1's rmse: 0.000403479\tvalid_1's RMSPE: 0.186974\n",
      "Early stopping, best iteration is:\n",
      "[3184]\ttraining's rmse: 0.000336783\ttraining's RMSPE: 0.155818\tvalid_1's rmse: 0.000403367\tvalid_1's RMSPE: 0.186922\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.0004294\ttraining's RMSPE: 0.198633\tvalid_1's rmse: 0.000439069\tvalid_1's RMSPE: 0.20361\n",
      "[500]\ttraining's rmse: 0.000407394\ttraining's RMSPE: 0.188454\tvalid_1's rmse: 0.000425216\tvalid_1's RMSPE: 0.197186\n",
      "[750]\ttraining's rmse: 0.000393509\ttraining's RMSPE: 0.182031\tvalid_1's rmse: 0.000417713\tvalid_1's RMSPE: 0.193707\n",
      "[1000]\ttraining's rmse: 0.0003834\ttraining's RMSPE: 0.177354\tvalid_1's rmse: 0.000413104\tvalid_1's RMSPE: 0.191569\n",
      "[1250]\ttraining's rmse: 0.000375386\ttraining's RMSPE: 0.173647\tvalid_1's rmse: 0.00041007\tvalid_1's RMSPE: 0.190162\n",
      "[1500]\ttraining's rmse: 0.000368809\ttraining's RMSPE: 0.170605\tvalid_1's rmse: 0.000408341\tvalid_1's RMSPE: 0.18936\n",
      "[1750]\ttraining's rmse: 0.000362802\ttraining's RMSPE: 0.167826\tvalid_1's rmse: 0.000407472\tvalid_1's RMSPE: 0.188958\n",
      "[2000]\ttraining's rmse: 0.000357451\ttraining's RMSPE: 0.165351\tvalid_1's rmse: 0.000406678\tvalid_1's RMSPE: 0.188589\n",
      "[2250]\ttraining's rmse: 0.000352592\ttraining's RMSPE: 0.163103\tvalid_1's rmse: 0.000405977\tvalid_1's RMSPE: 0.188264\n",
      "[2500]\ttraining's rmse: 0.000348049\ttraining's RMSPE: 0.161002\tvalid_1's rmse: 0.000405887\tvalid_1's RMSPE: 0.188223\n",
      "Early stopping, best iteration is:\n",
      "[2529]\ttraining's rmse: 0.000347541\ttraining's RMSPE: 0.160767\tvalid_1's rmse: 0.000405656\tvalid_1's RMSPE: 0.188115\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000428723\ttraining's RMSPE: 0.198515\tvalid_1's rmse: 0.000442297\tvalid_1's RMSPE: 0.204301\n",
      "[500]\ttraining's rmse: 0.000406991\ttraining's RMSPE: 0.188452\tvalid_1's rmse: 0.000429777\tvalid_1's RMSPE: 0.198518\n",
      "[750]\ttraining's rmse: 0.000393343\ttraining's RMSPE: 0.182133\tvalid_1's rmse: 0.000423513\tvalid_1's RMSPE: 0.195625\n",
      "[1000]\ttraining's rmse: 0.000383244\ttraining's RMSPE: 0.177457\tvalid_1's rmse: 0.000419682\tvalid_1's RMSPE: 0.193855\n",
      "[1250]\ttraining's rmse: 0.000375019\ttraining's RMSPE: 0.173648\tvalid_1's rmse: 0.000417168\tvalid_1's RMSPE: 0.192694\n",
      "[1500]\ttraining's rmse: 0.000368304\ttraining's RMSPE: 0.170539\tvalid_1's rmse: 0.000416003\tvalid_1's RMSPE: 0.192156\n",
      "[1750]\ttraining's rmse: 0.000362282\ttraining's RMSPE: 0.16775\tvalid_1's rmse: 0.000415006\tvalid_1's RMSPE: 0.191695\n",
      "[2000]\ttraining's rmse: 0.000357011\ttraining's RMSPE: 0.16531\tvalid_1's rmse: 0.000414512\tvalid_1's RMSPE: 0.191467\n",
      "[2250]\ttraining's rmse: 0.000352097\ttraining's RMSPE: 0.163034\tvalid_1's rmse: 0.000413845\tvalid_1's RMSPE: 0.191159\n",
      "[2500]\ttraining's rmse: 0.000347524\ttraining's RMSPE: 0.160917\tvalid_1's rmse: 0.000413812\tvalid_1's RMSPE: 0.191144\n",
      "Early stopping, best iteration is:\n",
      "[2405]\ttraining's rmse: 0.000349239\ttraining's RMSPE: 0.161711\tvalid_1's rmse: 0.000413485\tvalid_1's RMSPE: 0.190993\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000428801\ttraining's RMSPE: 0.198522\tvalid_1's rmse: 0.000441557\tvalid_1's RMSPE: 0.20408\n",
      "[500]\ttraining's rmse: 0.000406934\ttraining's RMSPE: 0.188398\tvalid_1's rmse: 0.000427962\tvalid_1's RMSPE: 0.197797\n",
      "[750]\ttraining's rmse: 0.000393259\ttraining's RMSPE: 0.182067\tvalid_1's rmse: 0.00042033\tvalid_1's RMSPE: 0.19427\n",
      "[1000]\ttraining's rmse: 0.000383196\ttraining's RMSPE: 0.177408\tvalid_1's rmse: 0.000415797\tvalid_1's RMSPE: 0.192175\n",
      "[1250]\ttraining's rmse: 0.000375131\ttraining's RMSPE: 0.173674\tvalid_1's rmse: 0.000413254\tvalid_1's RMSPE: 0.191\n",
      "[1500]\ttraining's rmse: 0.000368349\ttraining's RMSPE: 0.170534\tvalid_1's rmse: 0.000411401\tvalid_1's RMSPE: 0.190143\n",
      "[1750]\ttraining's rmse: 0.000362366\ttraining's RMSPE: 0.167764\tvalid_1's rmse: 0.000410088\tvalid_1's RMSPE: 0.189536\n",
      "[2000]\ttraining's rmse: 0.000357064\ttraining's RMSPE: 0.16531\tvalid_1's rmse: 0.000409223\tvalid_1's RMSPE: 0.189136\n",
      "[2250]\ttraining's rmse: 0.000352161\ttraining's RMSPE: 0.16304\tvalid_1's rmse: 0.000408566\tvalid_1's RMSPE: 0.188833\n",
      "[2500]\ttraining's rmse: 0.000347666\ttraining's RMSPE: 0.160959\tvalid_1's rmse: 0.000408125\tvalid_1's RMSPE: 0.188629\n",
      "[2750]\ttraining's rmse: 0.000343421\ttraining's RMSPE: 0.158994\tvalid_1's rmse: 0.000407652\tvalid_1's RMSPE: 0.18841\n",
      "[3000]\ttraining's rmse: 0.000339442\ttraining's RMSPE: 0.157151\tvalid_1's rmse: 0.000407255\tvalid_1's RMSPE: 0.188227\n",
      "[3250]\ttraining's rmse: 0.000335656\ttraining's RMSPE: 0.155399\tvalid_1's rmse: 0.000407176\tvalid_1's RMSPE: 0.18819\n",
      "[3500]\ttraining's rmse: 0.000332081\ttraining's RMSPE: 0.153743\tvalid_1's rmse: 0.000407152\tvalid_1's RMSPE: 0.188179\n",
      "Early stopping, best iteration is:\n",
      "[3423]\ttraining's rmse: 0.000333169\ttraining's RMSPE: 0.154247\tvalid_1's rmse: 0.000407031\tvalid_1's RMSPE: 0.188123\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000427322\ttraining's RMSPE: 0.197768\tvalid_1's rmse: 0.000473597\tvalid_1's RMSPE: 0.219196\n",
      "[500]\ttraining's rmse: 0.000405974\ttraining's RMSPE: 0.187888\tvalid_1's rmse: 0.000458636\tvalid_1's RMSPE: 0.212271\n",
      "[750]\ttraining's rmse: 0.000392434\ttraining's RMSPE: 0.181622\tvalid_1's rmse: 0.000451019\tvalid_1's RMSPE: 0.208746\n",
      "[1000]\ttraining's rmse: 0.000382418\ttraining's RMSPE: 0.176986\tvalid_1's rmse: 0.000444879\tvalid_1's RMSPE: 0.205904\n",
      "[1250]\ttraining's rmse: 0.000374399\ttraining's RMSPE: 0.173275\tvalid_1's rmse: 0.000441614\tvalid_1's RMSPE: 0.204393\n",
      "[1500]\ttraining's rmse: 0.000367819\ttraining's RMSPE: 0.170229\tvalid_1's rmse: 0.00044033\tvalid_1's RMSPE: 0.203799\n",
      "[1750]\ttraining's rmse: 0.000362022\ttraining's RMSPE: 0.167547\tvalid_1's rmse: 0.000439088\tvalid_1's RMSPE: 0.203224\n",
      "[2000]\ttraining's rmse: 0.000356635\ttraining's RMSPE: 0.165053\tvalid_1's rmse: 0.000437742\tvalid_1's RMSPE: 0.202601\n",
      "[2250]\ttraining's rmse: 0.000351872\ttraining's RMSPE: 0.162849\tvalid_1's rmse: 0.000436966\tvalid_1's RMSPE: 0.202242\n",
      "[2500]\ttraining's rmse: 0.000347405\ttraining's RMSPE: 0.160782\tvalid_1's rmse: 0.000437088\tvalid_1's RMSPE: 0.202298\n",
      "Early stopping, best iteration is:\n",
      "[2354]\ttraining's rmse: 0.000349918\ttraining's RMSPE: 0.161945\tvalid_1's rmse: 0.000436627\tvalid_1's RMSPE: 0.202085\n",
      "Our out of folds RMSPE is 0.19132906172848563\n",
      "For Seed: 2021\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000448628\ttraining's RMSPE: 0.207564\tvalid_1's rmse: 0.000454233\tvalid_1's RMSPE: 0.210493\n",
      "[500]\ttraining's rmse: 0.000432204\ttraining's RMSPE: 0.199966\tvalid_1's rmse: 0.000442283\tvalid_1's RMSPE: 0.204956\n",
      "[750]\ttraining's rmse: 0.000420031\ttraining's RMSPE: 0.194333\tvalid_1's rmse: 0.000433322\tvalid_1's RMSPE: 0.200803\n",
      "[1000]\ttraining's rmse: 0.000411031\ttraining's RMSPE: 0.190169\tvalid_1's rmse: 0.000427351\tvalid_1's RMSPE: 0.198036\n",
      "[1250]\ttraining's rmse: 0.000404108\ttraining's RMSPE: 0.186967\tvalid_1's rmse: 0.000423516\tvalid_1's RMSPE: 0.196259\n",
      "[1500]\ttraining's rmse: 0.000397819\ttraining's RMSPE: 0.184057\tvalid_1's rmse: 0.000420251\tvalid_1's RMSPE: 0.194746\n",
      "[1750]\ttraining's rmse: 0.000392596\ttraining's RMSPE: 0.18164\tvalid_1's rmse: 0.000417674\tvalid_1's RMSPE: 0.193552\n",
      "[2000]\ttraining's rmse: 0.000387917\ttraining's RMSPE: 0.179476\tvalid_1's rmse: 0.000415496\tvalid_1's RMSPE: 0.192543\n",
      "[2250]\ttraining's rmse: 0.000383613\ttraining's RMSPE: 0.177484\tvalid_1's rmse: 0.000413694\tvalid_1's RMSPE: 0.191707\n",
      "[2500]\ttraining's rmse: 0.000379832\ttraining's RMSPE: 0.175735\tvalid_1's rmse: 0.000412811\tvalid_1's RMSPE: 0.191298\n",
      "[2750]\ttraining's rmse: 0.000376573\ttraining's RMSPE: 0.174227\tvalid_1's rmse: 0.000412378\tvalid_1's RMSPE: 0.191098\n",
      "[3000]\ttraining's rmse: 0.000373541\ttraining's RMSPE: 0.172824\tvalid_1's rmse: 0.000411193\tvalid_1's RMSPE: 0.190549\n",
      "[3250]\ttraining's rmse: 0.000370635\ttraining's RMSPE: 0.17148\tvalid_1's rmse: 0.000410499\tvalid_1's RMSPE: 0.190227\n",
      "[3500]\ttraining's rmse: 0.000367858\ttraining's RMSPE: 0.170195\tvalid_1's rmse: 0.000409895\tvalid_1's RMSPE: 0.189947\n",
      "[3750]\ttraining's rmse: 0.000365155\ttraining's RMSPE: 0.168944\tvalid_1's rmse: 0.000409735\tvalid_1's RMSPE: 0.189873\n",
      "Early stopping, best iteration is:\n",
      "[3611]\ttraining's rmse: 0.000366573\ttraining's RMSPE: 0.169601\tvalid_1's rmse: 0.000409623\tvalid_1's RMSPE: 0.189821\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000449076\ttraining's RMSPE: 0.207735\tvalid_1's rmse: 0.000454692\tvalid_1's RMSPE: 0.210855\n",
      "[500]\ttraining's rmse: 0.000432735\ttraining's RMSPE: 0.200176\tvalid_1's rmse: 0.000442302\tvalid_1's RMSPE: 0.205109\n",
      "[750]\ttraining's rmse: 0.00042105\ttraining's RMSPE: 0.194771\tvalid_1's rmse: 0.00043368\tvalid_1's RMSPE: 0.201111\n",
      "[1000]\ttraining's rmse: 0.00041221\ttraining's RMSPE: 0.190681\tvalid_1's rmse: 0.000427735\tvalid_1's RMSPE: 0.198354\n",
      "[1250]\ttraining's rmse: 0.000404845\ttraining's RMSPE: 0.187274\tvalid_1's rmse: 0.00042338\tvalid_1's RMSPE: 0.196334\n",
      "[1500]\ttraining's rmse: 0.000398543\ttraining's RMSPE: 0.184359\tvalid_1's rmse: 0.000419464\tvalid_1's RMSPE: 0.194519\n",
      "[1750]\ttraining's rmse: 0.000393444\ttraining's RMSPE: 0.182\tvalid_1's rmse: 0.000417768\tvalid_1's RMSPE: 0.193732\n",
      "[2000]\ttraining's rmse: 0.000388694\ttraining's RMSPE: 0.179803\tvalid_1's rmse: 0.000415505\tvalid_1's RMSPE: 0.192683\n",
      "[2250]\ttraining's rmse: 0.000384375\ttraining's RMSPE: 0.177805\tvalid_1's rmse: 0.000413449\tvalid_1's RMSPE: 0.191729\n",
      "[2500]\ttraining's rmse: 0.000380556\ttraining's RMSPE: 0.176039\tvalid_1's rmse: 0.000412068\tvalid_1's RMSPE: 0.191089\n",
      "[2750]\ttraining's rmse: 0.000377192\ttraining's RMSPE: 0.174483\tvalid_1's rmse: 0.000410483\tvalid_1's RMSPE: 0.190354\n",
      "[3000]\ttraining's rmse: 0.000374101\ttraining's RMSPE: 0.173053\tvalid_1's rmse: 0.000409868\tvalid_1's RMSPE: 0.190068\n",
      "[3250]\ttraining's rmse: 0.000371111\ttraining's RMSPE: 0.17167\tvalid_1's rmse: 0.00040896\tvalid_1's RMSPE: 0.189647\n",
      "[3500]\ttraining's rmse: 0.000368433\ttraining's RMSPE: 0.170431\tvalid_1's rmse: 0.000408253\tvalid_1's RMSPE: 0.189319\n",
      "[3750]\ttraining's rmse: 0.000366027\ttraining's RMSPE: 0.169318\tvalid_1's rmse: 0.000407704\tvalid_1's RMSPE: 0.189065\n",
      "[4000]\ttraining's rmse: 0.000363509\ttraining's RMSPE: 0.168153\tvalid_1's rmse: 0.000407344\tvalid_1's RMSPE: 0.188898\n",
      "[4250]\ttraining's rmse: 0.000361189\ttraining's RMSPE: 0.16708\tvalid_1's rmse: 0.000407108\tvalid_1's RMSPE: 0.188789\n",
      "Early stopping, best iteration is:\n",
      "[4187]\ttraining's rmse: 0.000361745\ttraining's RMSPE: 0.167337\tvalid_1's rmse: 0.000407023\tvalid_1's RMSPE: 0.188749\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000448957\ttraining's RMSPE: 0.207884\tvalid_1's rmse: 0.000456783\tvalid_1's RMSPE: 0.210993\n",
      "[500]\ttraining's rmse: 0.00043158\ttraining's RMSPE: 0.199838\tvalid_1's rmse: 0.000444434\tvalid_1's RMSPE: 0.205288\n",
      "[750]\ttraining's rmse: 0.000419778\ttraining's RMSPE: 0.194373\tvalid_1's rmse: 0.000437164\tvalid_1's RMSPE: 0.20193\n",
      "[1000]\ttraining's rmse: 0.00041046\ttraining's RMSPE: 0.190059\tvalid_1's rmse: 0.000430886\tvalid_1's RMSPE: 0.19903\n",
      "[1250]\ttraining's rmse: 0.000403002\ttraining's RMSPE: 0.186606\tvalid_1's rmse: 0.000426419\tvalid_1's RMSPE: 0.196967\n",
      "[1500]\ttraining's rmse: 0.000396925\ttraining's RMSPE: 0.183791\tvalid_1's rmse: 0.000423602\tvalid_1's RMSPE: 0.195666\n",
      "[1750]\ttraining's rmse: 0.000391616\ttraining's RMSPE: 0.181333\tvalid_1's rmse: 0.000421062\tvalid_1's RMSPE: 0.194492\n",
      "[2000]\ttraining's rmse: 0.000387089\ttraining's RMSPE: 0.179237\tvalid_1's rmse: 0.000418932\tvalid_1's RMSPE: 0.193508\n",
      "[2250]\ttraining's rmse: 0.000383001\ttraining's RMSPE: 0.177344\tvalid_1's rmse: 0.000417946\tvalid_1's RMSPE: 0.193053\n",
      "[2500]\ttraining's rmse: 0.000379185\ttraining's RMSPE: 0.175577\tvalid_1's rmse: 0.000416573\tvalid_1's RMSPE: 0.192419\n",
      "[2750]\ttraining's rmse: 0.000375831\ttraining's RMSPE: 0.174024\tvalid_1's rmse: 0.000415632\tvalid_1's RMSPE: 0.191984\n",
      "[3000]\ttraining's rmse: 0.000372707\ttraining's RMSPE: 0.172578\tvalid_1's rmse: 0.000414875\tvalid_1's RMSPE: 0.191634\n",
      "[3250]\ttraining's rmse: 0.000369847\ttraining's RMSPE: 0.171253\tvalid_1's rmse: 0.000414328\tvalid_1's RMSPE: 0.191382\n",
      "[3500]\ttraining's rmse: 0.000367006\ttraining's RMSPE: 0.169938\tvalid_1's rmse: 0.000413617\tvalid_1's RMSPE: 0.191053\n",
      "[3750]\ttraining's rmse: 0.000364365\ttraining's RMSPE: 0.168715\tvalid_1's rmse: 0.000413303\tvalid_1's RMSPE: 0.190909\n",
      "Early stopping, best iteration is:\n",
      "[3738]\ttraining's rmse: 0.00036449\ttraining's RMSPE: 0.168773\tvalid_1's rmse: 0.000413119\tvalid_1's RMSPE: 0.190824\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000448033\ttraining's RMSPE: 0.207426\tvalid_1's rmse: 0.000455958\tvalid_1's RMSPE: 0.210736\n",
      "[500]\ttraining's rmse: 0.00043139\ttraining's RMSPE: 0.19972\tvalid_1's rmse: 0.000443424\tvalid_1's RMSPE: 0.204943\n",
      "[750]\ttraining's rmse: 0.000419794\ttraining's RMSPE: 0.194352\tvalid_1's rmse: 0.000435446\tvalid_1's RMSPE: 0.201256\n",
      "[1000]\ttraining's rmse: 0.000410775\ttraining's RMSPE: 0.190177\tvalid_1's rmse: 0.000429607\tvalid_1's RMSPE: 0.198558\n",
      "[1250]\ttraining's rmse: 0.000403893\ttraining's RMSPE: 0.18699\tvalid_1's rmse: 0.000425222\tvalid_1's RMSPE: 0.196531\n",
      "[1500]\ttraining's rmse: 0.000397794\ttraining's RMSPE: 0.184166\tvalid_1's rmse: 0.000421947\tvalid_1's RMSPE: 0.195017\n",
      "[1750]\ttraining's rmse: 0.000392556\ttraining's RMSPE: 0.181741\tvalid_1's rmse: 0.000419526\tvalid_1's RMSPE: 0.193898\n",
      "[2000]\ttraining's rmse: 0.00038808\ttraining's RMSPE: 0.179669\tvalid_1's rmse: 0.000417514\tvalid_1's RMSPE: 0.192968\n",
      "[2250]\ttraining's rmse: 0.000384037\ttraining's RMSPE: 0.177797\tvalid_1's rmse: 0.000415674\tvalid_1's RMSPE: 0.192118\n",
      "[2500]\ttraining's rmse: 0.000380167\ttraining's RMSPE: 0.176006\tvalid_1's rmse: 0.000413992\tvalid_1's RMSPE: 0.191341\n",
      "[2750]\ttraining's rmse: 0.000376507\ttraining's RMSPE: 0.174311\tvalid_1's rmse: 0.000412796\tvalid_1's RMSPE: 0.190788\n",
      "[3000]\ttraining's rmse: 0.000373496\ttraining's RMSPE: 0.172917\tvalid_1's rmse: 0.000411789\tvalid_1's RMSPE: 0.190322\n",
      "[3250]\ttraining's rmse: 0.000370593\ttraining's RMSPE: 0.171573\tvalid_1's rmse: 0.000411148\tvalid_1's RMSPE: 0.190026\n",
      "[3500]\ttraining's rmse: 0.000367886\ttraining's RMSPE: 0.17032\tvalid_1's rmse: 0.000410465\tvalid_1's RMSPE: 0.189711\n",
      "[3750]\ttraining's rmse: 0.000365232\ttraining's RMSPE: 0.169091\tvalid_1's rmse: 0.000409862\tvalid_1's RMSPE: 0.189432\n",
      "[4000]\ttraining's rmse: 0.000362698\ttraining's RMSPE: 0.167918\tvalid_1's rmse: 0.000409253\tvalid_1's RMSPE: 0.18915\n",
      "[4250]\ttraining's rmse: 0.000360476\ttraining's RMSPE: 0.16689\tvalid_1's rmse: 0.000409009\tvalid_1's RMSPE: 0.189037\n",
      "[4500]\ttraining's rmse: 0.000358278\ttraining's RMSPE: 0.165872\tvalid_1's rmse: 0.000408674\tvalid_1's RMSPE: 0.188883\n",
      "[4750]\ttraining's rmse: 0.000356188\ttraining's RMSPE: 0.164904\tvalid_1's rmse: 0.000408374\tvalid_1's RMSPE: 0.188744\n",
      "[5000]\ttraining's rmse: 0.000354333\ttraining's RMSPE: 0.164045\tvalid_1's rmse: 0.000408096\tvalid_1's RMSPE: 0.188615\n",
      "[5250]\ttraining's rmse: 0.000352299\ttraining's RMSPE: 0.163104\tvalid_1's rmse: 0.000407913\tvalid_1's RMSPE: 0.188531\n",
      "[5500]\ttraining's rmse: 0.000350363\ttraining's RMSPE: 0.162207\tvalid_1's rmse: 0.000407766\tvalid_1's RMSPE: 0.188463\n",
      "[5750]\ttraining's rmse: 0.000348618\ttraining's RMSPE: 0.1614\tvalid_1's rmse: 0.000407474\tvalid_1's RMSPE: 0.188328\n",
      "[6000]\ttraining's rmse: 0.000346925\ttraining's RMSPE: 0.160616\tvalid_1's rmse: 0.00040741\tvalid_1's RMSPE: 0.188298\n",
      "[6250]\ttraining's rmse: 0.000345344\ttraining's RMSPE: 0.159884\tvalid_1's rmse: 0.000407227\tvalid_1's RMSPE: 0.188214\n",
      "[6500]\ttraining's rmse: 0.000343659\ttraining's RMSPE: 0.159104\tvalid_1's rmse: 0.000407105\tvalid_1's RMSPE: 0.188157\n",
      "[6750]\ttraining's rmse: 0.000342025\ttraining's RMSPE: 0.158347\tvalid_1's rmse: 0.000406964\tvalid_1's RMSPE: 0.188092\n",
      "Early stopping, best iteration is:\n",
      "[6757]\ttraining's rmse: 0.00034197\ttraining's RMSPE: 0.158322\tvalid_1's rmse: 0.000406937\tvalid_1's RMSPE: 0.18808\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttraining's rmse: 0.000447149\ttraining's RMSPE: 0.206944\tvalid_1's rmse: 0.000487032\tvalid_1's RMSPE: 0.225414\n",
      "[500]\ttraining's rmse: 0.000430478\ttraining's RMSPE: 0.199228\tvalid_1's rmse: 0.000474352\tvalid_1's RMSPE: 0.219545\n",
      "[750]\ttraining's rmse: 0.000419211\ttraining's RMSPE: 0.194014\tvalid_1's rmse: 0.000466613\tvalid_1's RMSPE: 0.215963\n",
      "[1000]\ttraining's rmse: 0.000410802\ttraining's RMSPE: 0.190122\tvalid_1's rmse: 0.000461592\tvalid_1's RMSPE: 0.21364\n",
      "[1250]\ttraining's rmse: 0.000403206\ttraining's RMSPE: 0.186607\tvalid_1's rmse: 0.000455657\tvalid_1's RMSPE: 0.210893\n",
      "[1500]\ttraining's rmse: 0.000396932\ttraining's RMSPE: 0.183703\tvalid_1's rmse: 0.000451833\tvalid_1's RMSPE: 0.209123\n",
      "[1750]\ttraining's rmse: 0.000391683\ttraining's RMSPE: 0.181274\tvalid_1's rmse: 0.000448723\tvalid_1's RMSPE: 0.207683\n",
      "[2000]\ttraining's rmse: 0.00038717\ttraining's RMSPE: 0.179185\tvalid_1's rmse: 0.000446924\tvalid_1's RMSPE: 0.20685\n",
      "[2250]\ttraining's rmse: 0.000383335\ttraining's RMSPE: 0.177411\tvalid_1's rmse: 0.000445676\tvalid_1's RMSPE: 0.206273\n",
      "[2500]\ttraining's rmse: 0.000379555\ttraining's RMSPE: 0.175661\tvalid_1's rmse: 0.000443427\tvalid_1's RMSPE: 0.205232\n",
      "[2750]\ttraining's rmse: 0.000376235\ttraining's RMSPE: 0.174124\tvalid_1's rmse: 0.000441897\tvalid_1's RMSPE: 0.204524\n",
      "[3000]\ttraining's rmse: 0.00037288\ttraining's RMSPE: 0.172572\tvalid_1's rmse: 0.000440709\tvalid_1's RMSPE: 0.203974\n",
      "[3250]\ttraining's rmse: 0.000369923\ttraining's RMSPE: 0.171203\tvalid_1's rmse: 0.000439622\tvalid_1's RMSPE: 0.203471\n",
      "[3500]\ttraining's rmse: 0.00036725\ttraining's RMSPE: 0.169966\tvalid_1's rmse: 0.000439128\tvalid_1's RMSPE: 0.203242\n",
      "[3750]\ttraining's rmse: 0.000364857\ttraining's RMSPE: 0.168859\tvalid_1's rmse: 0.000438301\tvalid_1's RMSPE: 0.20286\n",
      "[4000]\ttraining's rmse: 0.000362524\ttraining's RMSPE: 0.167779\tvalid_1's rmse: 0.000437858\tvalid_1's RMSPE: 0.202655\n",
      "[4250]\ttraining's rmse: 0.000360242\ttraining's RMSPE: 0.166723\tvalid_1's rmse: 0.000436604\tvalid_1's RMSPE: 0.202074\n",
      "[4500]\ttraining's rmse: 0.000357988\ttraining's RMSPE: 0.16568\tvalid_1's rmse: 0.00043632\tvalid_1's RMSPE: 0.201943\n",
      "[4750]\ttraining's rmse: 0.000355837\ttraining's RMSPE: 0.164684\tvalid_1's rmse: 0.000435964\tvalid_1's RMSPE: 0.201778\n",
      "[5000]\ttraining's rmse: 0.000353878\ttraining's RMSPE: 0.163778\tvalid_1's rmse: 0.000435354\tvalid_1's RMSPE: 0.201496\n",
      "[5250]\ttraining's rmse: 0.000351865\ttraining's RMSPE: 0.162846\tvalid_1's rmse: 0.00043443\tvalid_1's RMSPE: 0.201068\n",
      "[5500]\ttraining's rmse: 0.000349904\ttraining's RMSPE: 0.161938\tvalid_1's rmse: 0.000433904\tvalid_1's RMSPE: 0.200825\n",
      "[5750]\ttraining's rmse: 0.000348154\ttraining's RMSPE: 0.161128\tvalid_1's rmse: 0.000433735\tvalid_1's RMSPE: 0.200746\n",
      "[6000]\ttraining's rmse: 0.000346392\ttraining's RMSPE: 0.160313\tvalid_1's rmse: 0.000433704\tvalid_1's RMSPE: 0.200732\n",
      "Early stopping, best iteration is:\n",
      "[5824]\ttraining's rmse: 0.000347647\ttraining's RMSPE: 0.160894\tvalid_1's rmse: 0.000433476\tvalid_1's RMSPE: 0.200627\n",
      "Our out of folds RMSPE is 0.19167532671001677\n"
     ]
    }
   ],
   "source": [
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params, seed):\n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "   \n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    \n",
    "    kfold = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight=train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight=val_weights)\n",
    "        model = lgb.train(params=params,\n",
    "                          num_boost_round=10000,\n",
    "                          train_set=train_dataset, \n",
    "                          valid_sets=[train_dataset, val_dataset], \n",
    "                          verbose_eval=250,\n",
    "                          early_stopping_rounds=200,\n",
    "                          feval=feval_rmspe)\n",
    "        \n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    return test_predictions\n",
    "\n",
    "print(f\"For Seed: {seed0}\")\n",
    "predictions_lgb0 = train_and_evaluate_lgb(train, test, params69, seed0)\n",
    "print(f\"For Seed: {seed1}\")\n",
    "predictions_lgb1 = train_and_evaluate_lgb(train, test, params2021, seed1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d542d877",
   "metadata": {
    "papermill": {
     "duration": 0.078099,
     "end_time": "2021-09-18T13:08:49.199103",
     "exception": false,
     "start_time": "2021-09-18T13:08:49.121004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a290f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:08:49.378730Z",
     "iopub.status.busy": "2021-09-18T13:08:49.377956Z",
     "iopub.status.idle": "2021-09-18T13:08:56.017194Z",
     "shell.execute_reply": "2021-09-18T13:08:56.016609Z"
    },
    "papermill": {
     "duration": 6.740247,
     "end_time": "2021-09-18T13:08:56.017352",
     "exception": false,
     "start_time": "2021-09-18T13:08:49.277105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56b0cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:08:56.183024Z",
     "iopub.status.busy": "2021-09-18T13:08:56.182368Z",
     "iopub.status.idle": "2021-09-18T13:08:56.184241Z",
     "shell.execute_reply": "2021-09-18T13:08:56.184748Z"
    },
    "papermill": {
     "duration": 0.0893,
     "end_time": "2021-09-18T13:08:56.184936",
     "exception": false,
     "start_time": "2021-09-18T13:08:56.095636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square((y_true - y_pred)/y_true)))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    verbose=0,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=7,\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb962f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:08:56.346555Z",
     "iopub.status.busy": "2021-09-18T13:08:56.345851Z",
     "iopub.status.idle": "2021-09-18T13:09:08.595267Z",
     "shell.execute_reply": "2021-09-18T13:09:08.594636Z"
    },
    "papermill": {
     "duration": 12.330831,
     "end_time": "2021-09-18T13:09:08.595420",
     "exception": false,
     "start_time": "2021-09-18T13:08:56.264589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.008067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.004670</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.003965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.003161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.003593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.003496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id       0         1         2         3         4         5    \\\n",
       "time_id                                                                \n",
       "5         0.004136  0.006340  0.001848  0.005300  0.004468  0.006234   \n",
       "11        0.001445  0.002099  0.000806  0.002774  0.001852  0.002562   \n",
       "16        0.002168  0.002456  0.001581  0.002986  0.002213  0.003253   \n",
       "31        0.002195  0.002807  0.001599  0.004437  0.002256  0.003072   \n",
       "62        0.001747  0.004312  0.001503  0.003408  0.002102  0.002824   \n",
       "\n",
       "stock_id       6         7         8         9    ...       115       116  \\\n",
       "time_id                                           ...                       \n",
       "5         0.007651  0.003624  0.010036  0.007291  ...  0.004267  0.007944   \n",
       "11        0.004670  0.002458  0.002291  0.002529  ...  0.001382  0.002469   \n",
       "16        0.004303  0.002178  0.001841  0.003299  ...  0.001949  0.002195   \n",
       "31        0.005401  0.002149  0.003997  0.003696  ...  0.002035  0.002298   \n",
       "62        0.004562  0.002203  0.003923  0.003689  ...  0.002459  0.003704   \n",
       "\n",
       "stock_id       118       119       120       122       123       124  \\\n",
       "time_id                                                                \n",
       "5         0.003336  0.002571  0.003035  0.004862  0.002942  0.004112   \n",
       "11        0.002030  0.000839  0.001271  0.002095  0.001518  0.001891   \n",
       "16        0.003410  0.002569  0.002137  0.001893  0.002131  0.002428   \n",
       "31        0.005674  0.002115  0.001734  0.003509  0.001078  0.002182   \n",
       "62        0.003914  0.001549  0.001470  0.002151  0.001253  0.002382   \n",
       "\n",
       "stock_id       125       126  \n",
       "time_id                       \n",
       "5         0.001919  0.008067  \n",
       "11        0.001123  0.003965  \n",
       "16        0.001548  0.003161  \n",
       "31        0.001251  0.003593  \n",
       "62        0.001324  0.003496  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "display(out_train.head())\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat, np.arange(mat.shape[0])]\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "s = []\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n], :])\n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "    luck = np.random.uniform(0, 1, nfolds)\n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ddb2506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:08.765010Z",
     "iopub.status.busy": "2021-09-18T13:09:08.764147Z",
     "iopub.status.idle": "2021-09-18T13:09:37.332857Z",
     "shell.execute_reply": "2021-09-18T13:09:37.332275Z"
    },
    "papermill": {
     "duration": 28.656297,
     "end_time": "2021-09-18T13:09:37.333036",
     "exception": false,
     "start_time": "2021-09-18T13:09:08.676739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "qt_train = []\n",
    "train_nn = train[colNames].copy()\n",
    "test_nn = test[colNames].copy()\n",
    "\n",
    "for col in colNames:\n",
    "    qt = QuantileTransformer(random_state=69, n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d0fb625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:37.501549Z",
     "iopub.status.busy": "2021-09-18T13:09:37.500826Z",
     "iopub.status.idle": "2021-09-18T13:09:37.509902Z",
     "shell.execute_reply": "2021-09-18T13:09:37.510385Z"
    },
    "papermill": {
     "duration": 0.098654,
     "end_time": "2021-09-18T13:09:37.510570",
     "exception": false,
     "start_time": "2021-09-18T13:09:37.411916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id', 'time_id', 'target']] = train[['stock_id', 'time_id', 'target']]\n",
    "test_nn[['stock_id', 'time_id']] = test[['stock_id', 'time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7385b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:37.685979Z",
     "iopub.status.busy": "2021-09-18T13:09:37.683091Z",
     "iopub.status.idle": "2021-09-18T13:09:39.430910Z",
     "shell.execute_reply": "2021-09-18T13:09:39.430369Z"
    },
    "papermill": {
     "duration": 1.840345,
     "end_time": "2021-09-18T13:09:39.431072",
     "exception": false,
     "start_time": "2021-09-18T13:09:37.590727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ([(x-1) for x in ((ids+1)*(kmeans.labels_ == n)) if x > 0])\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "\n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'], inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2, mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cde792e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:39.596988Z",
     "iopub.status.busy": "2021-09-18T13:09:39.595918Z",
     "iopub.status.idle": "2021-09-18T13:09:39.598421Z",
     "shell.execute_reply": "2021-09-18T13:09:39.598935Z"
    },
    "papermill": {
     "duration": 0.089362,
     "end_time": "2021-09-18T13:09:39.599109",
     "exception": false,
     "start_time": "2021-09-18T13:09:39.509747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c749d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:39.764009Z",
     "iopub.status.busy": "2021-09-18T13:09:39.763327Z",
     "iopub.status.idle": "2021-09-18T13:09:39.953004Z",
     "shell.execute_reply": "2021-09-18T13:09:39.952395Z"
    },
    "papermill": {
     "duration": 0.272725,
     "end_time": "2021-09-18T13:09:39.953169",
     "exception": false,
     "start_time": "2021-09-18T13:09:39.680444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a931c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:40.124033Z",
     "iopub.status.busy": "2021-09-18T13:09:40.123345Z",
     "iopub.status.idle": "2021-09-18T13:09:45.844091Z",
     "shell.execute_reply": "2021-09-18T13:09:45.843509Z"
    },
    "papermill": {
     "duration": 5.809405,
     "end_time": "2021-09-18T13:09:45.844249",
     "exception": false,
     "start_time": "2021-09-18T13:09:40.034844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn = pd.merge(train_nn, mat1[nnn], how='left', on='time_id')\n",
    "test_nn = pd.merge(test_nn, mat2[nnn], how='left', on='time_id')\n",
    "\n",
    "train1 = train_nn.copy()\n",
    "test1 = test_nn.copy()\n",
    "del mat1, mat2\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35bee450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:46.016619Z",
     "iopub.status.busy": "2021-09-18T13:09:46.015914Z",
     "iopub.status.idle": "2021-09-18T13:09:46.030685Z",
     "shell.execute_reply": "2021-09-18T13:09:46.029378Z"
    },
    "papermill": {
     "duration": 0.105686,
     "end_time": "2021-09-18T13:09:46.030860",
     "exception": false,
     "start_time": "2021-09-18T13:09:45.925174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def swish(x, beta=0.8):\n",
    "    return (x * K.sigmoid(beta * x))\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (192, 128, 64, 32)\n",
    "stock_embedding_size = 32\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1,\n",
    "                                            stock_embedding_size,\n",
    "                                            input_length=1,\n",
    "                                            name='stock_embedding')(stock_id_input)\n",
    "    \n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    for n_hidden in hidden_units:\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=[stock_id_input, num_input],\n",
    "        outputs=out\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7869c1e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:09:46.210215Z",
     "iopub.status.busy": "2021-09-18T13:09:46.209530Z",
     "iopub.status.idle": "2021-09-18T13:27:06.900916Z",
     "shell.execute_reply": "2021-09-18T13:27:06.900244Z"
    },
    "papermill": {
     "duration": 1040.789105,
     "end_time": "2021-09-18T13:27:06.901069",
     "exception": false,
     "start_time": "2021-09-18T13:09:46.111964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 6s 14ms/step - loss: 11.5378 - val_loss: 0.6783\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8265 - val_loss: 0.2583\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4678 - val_loss: 0.3868\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4070 - val_loss: 0.4269\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3842 - val_loss: 0.2264\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3193 - val_loss: 0.3447\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3011 - val_loss: 0.2673\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2580 - val_loss: 0.5275\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.3494 - val_loss: 0.3586\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3035 - val_loss: 0.2474\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2397 - val_loss: 0.2404\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2324 - val_loss: 0.3188\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2209 - val_loss: 0.2165\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2116 - val_loss: 0.2192\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2150 - val_loss: 0.2190\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2130 - val_loss: 0.2170\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2107 - val_loss: 0.2176\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2113 - val_loss: 0.2148\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2110 - val_loss: 0.2168\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2156 - val_loss: 0.2175\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2141 - val_loss: 0.2327\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2143 - val_loss: 0.2221\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2112 - val_loss: 0.2166\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2190 - val_loss: 0.2149\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2112 - val_loss: 0.2366\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2072 - val_loss: 0.2109\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2044 - val_loss: 0.2125\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2040 - val_loss: 0.2127\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2041 - val_loss: 0.2122\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2043 - val_loss: 0.2125\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2042 - val_loss: 0.2137\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2052 - val_loss: 0.2118\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2049 - val_loss: 0.2120\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2026 - val_loss: 0.2106\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2030 - val_loss: 0.2107\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2026 - val_loss: 0.2107\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2027 - val_loss: 0.2110\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2030 - val_loss: 0.2119\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2025 - val_loss: 0.2137\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2025 - val_loss: 0.2132\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2028 - val_loss: 0.2113\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.280000142287463e-05.\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2013 - val_loss: 0.2108\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2017 - val_loss: 0.2106\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2021 - val_loss: 0.2106\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2017 - val_loss: 0.2107\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2018 - val_loss: 0.2106\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2024 - val_loss: 0.2108\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2018 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-06.\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2019 - val_loss: 0.2107\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2011 - val_loss: 0.2107\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2016 - val_loss: 0.2107\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2017 - val_loss: 0.2107\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2014 - val_loss: 0.2108\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2018 - val_loss: 0.2107\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2011 - val_loss: 0.2108\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-07.\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2013 - val_loss: 0.2107\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2014 - val_loss: 0.2107\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2024 - val_loss: 0.2107\n",
      "Epoch 59/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2011 - val_loss: 0.2107\n",
      "Fold 1 NN_minmax: 0.21062\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 19.4466 - val_loss: 0.6155\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.9959 - val_loss: 0.6298\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.7194 - val_loss: 0.5749\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6060 - val_loss: 0.2349\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.3872 - val_loss: 0.3547\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4157 - val_loss: 0.2246\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2805 - val_loss: 1.3185\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4582 - val_loss: 0.2455\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2322 - val_loss: 0.2986\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2466 - val_loss: 1.5210\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.9838 - val_loss: 0.2428\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2628 - val_loss: 0.2271\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2819 - val_loss: 0.4369\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2305 - val_loss: 0.2157\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2131 - val_loss: 0.2171\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2124 - val_loss: 0.2194\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2141 - val_loss: 0.2186\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2135 - val_loss: 0.2221\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2138 - val_loss: 0.2182\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2115 - val_loss: 0.2162\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2135 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2099 - val_loss: 0.2183\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2083 - val_loss: 0.2136\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2072 - val_loss: 0.2160\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2064 - val_loss: 0.2133\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2076 - val_loss: 0.2152\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2070 - val_loss: 0.2136\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2077 - val_loss: 0.2139\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2075 - val_loss: 0.2181\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2071 - val_loss: 0.2147\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2071 - val_loss: 0.2137\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2060 - val_loss: 0.2210\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2054 - val_loss: 0.2130\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2047 - val_loss: 0.2125\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2042 - val_loss: 0.2124\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2040 - val_loss: 0.2132\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2042 - val_loss: 0.2129\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2033 - val_loss: 0.2124\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2040 - val_loss: 0.2129\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2037 - val_loss: 0.2125\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2035 - val_loss: 0.2132\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.280000142287463e-05.\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2038 - val_loss: 0.2126\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2032 - val_loss: 0.2126\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2027 - val_loss: 0.2124\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2027 - val_loss: 0.2127\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2027 - val_loss: 0.2125\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2037 - val_loss: 0.2127\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2030 - val_loss: 0.2125\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-06.\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2035 - val_loss: 0.2126\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2027 - val_loss: 0.2126\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2026 - val_loss: 0.2127\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2033 - val_loss: 0.2128\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2023 - val_loss: 0.2125\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2031 - val_loss: 0.2123\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2036 - val_loss: 0.2125\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2031 - val_loss: 0.2124\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2032 - val_loss: 0.2126\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2032 - val_loss: 0.2124\n",
      "Epoch 59/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2025 - val_loss: 0.2124\n",
      "Epoch 60/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2025 - val_loss: 0.2124\n",
      "Epoch 61/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2025 - val_loss: 0.2124\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-07.\n",
      "Epoch 62/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2025 - val_loss: 0.2124\n",
      "Epoch 63/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2033 - val_loss: 0.2125\n",
      "Epoch 64/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2028 - val_loss: 0.2125\n",
      "Epoch 65/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2027 - val_loss: 0.2124\n",
      "Epoch 66/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2025 - val_loss: 0.2125\n",
      "Epoch 67/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2029 - val_loss: 0.2125\n",
      "Epoch 68/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2027 - val_loss: 0.2124\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-07.\n",
      "Epoch 69/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2030 - val_loss: 0.2125\n",
      "Fold 2 NN_minmax: 0.21234\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 15.9737 - val_loss: 0.2907\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.6299 - val_loss: 0.6475\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5734 - val_loss: 0.4610\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.4071 - val_loss: 0.4065\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.3297 - val_loss: 0.4537\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3173 - val_loss: 0.2315\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3174 - val_loss: 0.2299\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.3001 - val_loss: 0.2384\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3180 - val_loss: 0.2210\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2824 - val_loss: 0.3288\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3185 - val_loss: 0.2170\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2919 - val_loss: 0.2263\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2384 - val_loss: 0.2504\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2778 - val_loss: 0.8925\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.7947 - val_loss: 0.4635\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3170 - val_loss: 0.2185\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.3012 - val_loss: 0.4577\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2932 - val_loss: 0.2680\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2163 - val_loss: 0.2117\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2108 - val_loss: 0.2129\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2113 - val_loss: 0.2120\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2125 - val_loss: 0.2100\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2104 - val_loss: 0.2084\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2103 - val_loss: 0.2134\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2117 - val_loss: 0.2150\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2149 - val_loss: 0.2090\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2105 - val_loss: 0.2144\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2125 - val_loss: 0.2368\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2137 - val_loss: 0.2141\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2088 - val_loss: 0.2146\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2051 - val_loss: 0.2078\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2036 - val_loss: 0.2079\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2040 - val_loss: 0.2094\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2034 - val_loss: 0.2102\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2037 - val_loss: 0.2097\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2035 - val_loss: 0.2132\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2041 - val_loss: 0.2118\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2038 - val_loss: 0.2083\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2023 - val_loss: 0.2076\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2022 - val_loss: 0.2079\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2014 - val_loss: 0.2080\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2023 - val_loss: 0.2080\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2013 - val_loss: 0.2077\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2016 - val_loss: 0.2079\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2018 - val_loss: 0.2082\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2018 - val_loss: 0.2082\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.280000142287463e-05.\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2011 - val_loss: 0.2074\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2016 - val_loss: 0.2074\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2009 - val_loss: 0.2078\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2016 - val_loss: 0.2079\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2010 - val_loss: 0.2075\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2012 - val_loss: 0.2078\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2013 - val_loss: 0.2077\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 0.2015 - val_loss: 0.2076\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-06.\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2010 - val_loss: 0.2077\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2015 - val_loss: 0.2078\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2016 - val_loss: 0.2079\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2015 - val_loss: 0.2078\n",
      "Epoch 59/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2011 - val_loss: 0.2076\n",
      "Epoch 60/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2004 - val_loss: 0.2077\n",
      "Epoch 61/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2010 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-07.\n",
      "Epoch 62/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2006 - val_loss: 0.2076\n",
      "Fold 3 NN_minmax: 0.2074\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 14.4724 - val_loss: 1.8854\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.9669 - val_loss: 0.4742\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6351 - val_loss: 0.3991\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4653 - val_loss: 0.3243\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2641 - val_loss: 0.3131\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3052 - val_loss: 0.2941\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2486 - val_loss: 0.3466\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2615 - val_loss: 0.2408\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2731 - val_loss: 0.2601\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.4343 - val_loss: 0.3522\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2803 - val_loss: 0.3351\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2692 - val_loss: 0.2503\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2383 - val_loss: 0.2645\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2387 - val_loss: 0.2261\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3102 - val_loss: 0.2337\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2435 - val_loss: 0.4262\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2671 - val_loss: 0.2703\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2419 - val_loss: 0.2437\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2503 - val_loss: 0.3140\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 15.0211 - val_loss: 0.5803\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5705 - val_loss: 0.5876\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5560 - val_loss: 0.5631\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.5547 - val_loss: 0.6284\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 5s 15ms/step - loss: 0.5612 - val_loss: 0.5622\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.5517 - val_loss: 0.5738\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5600 - val_loss: 0.5606\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5541 - val_loss: 0.5656\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5542 - val_loss: 0.5644\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5513 - val_loss: 0.5614\n",
      "Fold 4 NN_minmax: 0.22608\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 13.9031 - val_loss: 0.9404\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.8765 - val_loss: 0.6572\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5795 - val_loss: 0.5204\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3217 - val_loss: 1.7597\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8201 - val_loss: 0.2806\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.3820 - val_loss: 0.3606\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3273 - val_loss: 0.3567\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3054 - val_loss: 0.2285\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2378 - val_loss: 0.2765\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2347 - val_loss: 0.2655\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2561 - val_loss: 0.2648\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4112 - val_loss: 0.2597\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2552 - val_loss: 0.2805\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.4078 - val_loss: 0.2741\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2342 - val_loss: 0.2276\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2794 - val_loss: 0.2230\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 0.2424 - val_loss: 0.2163\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2394 - val_loss: 0.2183\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2429 - val_loss: 0.2800\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 8.3650 - val_loss: 0.5612\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5671 - val_loss: 0.5502\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5668 - val_loss: 0.5494\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5692 - val_loss: 0.5253\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5552 - val_loss: 0.4904\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4585 - val_loss: 0.4289\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.4066 - val_loss: 0.2850\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 5s 15ms/step - loss: 0.2866 - val_loss: 0.2663\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2675 - val_loss: 0.2583\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2589 - val_loss: 0.3108\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2490 - val_loss: 0.2744\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2442 - val_loss: 0.2385\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2315 - val_loss: 0.2352\n",
      "Fold 5 NN_minmax: 0.21626\n"
     ]
    }
   ],
   "source": [
    "target_name = 'target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN_minmax'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN_minmax')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes, obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]], values[indexes[1]], values[indexes[2]], values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.008),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=1024,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1, -1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true=y_test, y_pred=preds), 5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt = scaler.transform(test_nn[features_to_consider].values)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0, 1e10)/n_folds\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82217bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:27:17.751550Z",
     "iopub.status.busy": "2021-09-18T13:27:17.746191Z",
     "iopub.status.idle": "2021-09-18T13:48:35.614088Z",
     "shell.execute_reply": "2021-09-18T13:48:35.614984Z"
    },
    "papermill": {
     "duration": 1283.282571,
     "end_time": "2021-09-18T13:48:35.615247",
     "exception": false,
     "start_time": "2021-09-18T13:27:12.332676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 12ms/step - loss: 87.6385 - val_loss: 4.8607\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 3.6117 - val_loss: 2.5908\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 2.6367 - val_loss: 4.1114\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 2.6620 - val_loss: 1.1806\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 2.0877 - val_loss: 3.1501\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.6258 - val_loss: 2.2941\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 1.3407 - val_loss: 0.6672\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.0572 - val_loss: 1.0872\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.0617 - val_loss: 0.9543\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.9061 - val_loss: 0.4398\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8366 - val_loss: 1.2976\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.0071 - val_loss: 0.8946\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.9215 - val_loss: 0.7694\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.8779 - val_loss: 1.0126\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.8424 - val_loss: 0.8085\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.8651 - val_loss: 0.4788\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 2.5753 - val_loss: 1.1428\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6143 - val_loss: 0.5538\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5638 - val_loss: 0.5392\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5219 - val_loss: 0.3739\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3687 - val_loss: 0.3390\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.3377 - val_loss: 0.3548\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3559 - val_loss: 0.3294\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3252 - val_loss: 0.2669\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2793 - val_loss: 0.4138\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2949 - val_loss: 0.2561\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2649 - val_loss: 0.4197\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3165 - val_loss: 0.3787\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2719 - val_loss: 0.2630\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2713 - val_loss: 0.4589\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2945 - val_loss: 0.2899\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2878 - val_loss: 0.4188\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.3231 - val_loss: 0.2473\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2648 - val_loss: 0.2514\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2800 - val_loss: 0.2557\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2513 - val_loss: 0.3167\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2633 - val_loss: 0.2379\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2536 - val_loss: 0.2925\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2588 - val_loss: 0.2444\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2543 - val_loss: 0.3443\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3061 - val_loss: 0.2537\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2453 - val_loss: 0.3713\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2512 - val_loss: 0.2370\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3077 - val_loss: 0.2429\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2470 - val_loss: 0.2444\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2430 - val_loss: 0.2412\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2660 - val_loss: 0.3551\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2716 - val_loss: 0.2731\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2445 - val_loss: 0.2548\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2412 - val_loss: 0.2368\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2459 - val_loss: 0.2723\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3279 - val_loss: 0.2343\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2371 - val_loss: 0.2982\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2406 - val_loss: 0.3282\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2934 - val_loss: 0.2333\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2394 - val_loss: 0.2415\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2392 - val_loss: 0.2733\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2665 - val_loss: 0.2621\n",
      "Epoch 59/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2363 - val_loss: 0.2333\n",
      "Epoch 60/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2435 - val_loss: 0.2529\n",
      "Epoch 61/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2537 - val_loss: 0.2318\n",
      "Epoch 62/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2289 - val_loss: 0.2350\n",
      "Epoch 63/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2286 - val_loss: 0.2732\n",
      "Epoch 64/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2463 - val_loss: 0.2386\n",
      "Epoch 65/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2424 - val_loss: 0.2315\n",
      "Epoch 66/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2362 - val_loss: 0.3539\n",
      "Epoch 67/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2718 - val_loss: 0.3653\n",
      "Epoch 68/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2803 - val_loss: 0.2333\n",
      "Epoch 69/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2330 - val_loss: 0.2473\n",
      "Epoch 70/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2291 - val_loss: 0.2367\n",
      "Epoch 71/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2350 - val_loss: 0.2314\n",
      "Epoch 72/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2298 - val_loss: 0.2317\n",
      "Epoch 73/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2734 - val_loss: 0.2678\n",
      "Epoch 74/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2350 - val_loss: 0.2325\n",
      "Epoch 75/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2361 - val_loss: 0.2425\n",
      "Epoch 76/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2248 - val_loss: 0.2379\n",
      "Epoch 77/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2372 - val_loss: 0.2337\n",
      "Epoch 78/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2285 - val_loss: 0.2345\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 79/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2143 - val_loss: 0.2328\n",
      "Epoch 80/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2147 - val_loss: 0.2370\n",
      "Epoch 81/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2136 - val_loss: 0.2329\n",
      "Epoch 82/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2126 - val_loss: 0.2338\n",
      "Epoch 83/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2151 - val_loss: 0.2355\n",
      "Epoch 84/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2138 - val_loss: 0.2363\n",
      "Epoch 85/1000\n",
      "336/336 [==============================] - 5s 15ms/step - loss: 0.2141 - val_loss: 0.2357\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 86/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2118 - val_loss: 0.2338\n",
      "Fold 1 NN_ss: 0.23137\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 59.2395 - val_loss: 1.9393\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.6988 - val_loss: 1.2023\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.0530 - val_loss: 0.6314\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6861 - val_loss: 0.6210\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8588 - val_loss: 0.5119\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8685 - val_loss: 0.2392\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4142 - val_loss: 0.4977\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.0238 - val_loss: 0.5863\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5421 - val_loss: 0.9017\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8509 - val_loss: 0.2710\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3202 - val_loss: 0.2258\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3141 - val_loss: 0.3500\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.3348 - val_loss: 0.2284\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 0.2561 - val_loss: 0.5372\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 5s 15ms/step - loss: 0.3779 - val_loss: 0.2478\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.3901 - val_loss: 1.1338\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6409 - val_loss: 0.5570\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5704 - val_loss: 0.7171\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5678 - val_loss: 0.4801\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5319 - val_loss: 0.5131\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4100 - val_loss: 0.2680\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2603 - val_loss: 0.4266\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2603 - val_loss: 0.2630\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2350 - val_loss: 0.2613\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2349 - val_loss: 0.2311\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2188 - val_loss: 0.2228\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2163 - val_loss: 0.2209\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2164 - val_loss: 0.2209\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2153 - val_loss: 0.2304\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2160 - val_loss: 0.2201\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2150 - val_loss: 0.2206\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2130 - val_loss: 0.2218\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2144 - val_loss: 0.2215\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2128 - val_loss: 0.2216\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2129 - val_loss: 0.2194\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2111 - val_loss: 0.2218\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2115 - val_loss: 0.2181\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2108 - val_loss: 0.2239\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2120 - val_loss: 0.2166\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2106 - val_loss: 0.2367\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2107 - val_loss: 0.2367\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2133 - val_loss: 0.2269\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2100 - val_loss: 0.2186\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2086 - val_loss: 0.2184\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2103 - val_loss: 0.2196\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2080 - val_loss: 0.2225\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2073 - val_loss: 0.2156\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2044 - val_loss: 0.2161\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2056 - val_loss: 0.2167\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2046 - val_loss: 0.2161\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2043 - val_loss: 0.2160\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2046 - val_loss: 0.2164\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2041 - val_loss: 0.2184\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2048 - val_loss: 0.2162\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.280000142287463e-05.\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2045 - val_loss: 0.2155\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2038 - val_loss: 0.2154\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2038 - val_loss: 0.2162\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2039 - val_loss: 0.2160\n",
      "Epoch 59/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2032 - val_loss: 0.2156\n",
      "Epoch 60/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2036 - val_loss: 0.2157\n",
      "Epoch 61/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2037 - val_loss: 0.2161\n",
      "Epoch 62/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2033 - val_loss: 0.2162\n",
      "Epoch 63/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2041 - val_loss: 0.2155\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-06.\n",
      "Epoch 64/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2033 - val_loss: 0.2155\n",
      "Epoch 65/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2031 - val_loss: 0.2154\n",
      "Epoch 66/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2030 - val_loss: 0.2156\n",
      "Epoch 67/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2036 - val_loss: 0.2156\n",
      "Epoch 68/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2031 - val_loss: 0.2154\n",
      "Epoch 69/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2033 - val_loss: 0.2155\n",
      "Epoch 70/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2030 - val_loss: 0.2155\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-07.\n",
      "Epoch 71/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2027 - val_loss: 0.2156\n",
      "Fold 2 NN_ss: 0.21536\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 53.9966 - val_loss: 6.8378\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 3.1775 - val_loss: 4.1404\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 2.2654 - val_loss: 1.1090\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.9395 - val_loss: 0.4035\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.7150 - val_loss: 0.6342\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5673 - val_loss: 0.8964\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5955 - val_loss: 0.5079\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6005 - val_loss: 0.3165\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3193 - val_loss: 0.3392\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5844 - val_loss: 0.5862\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5189 - val_loss: 0.4996\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5145 - val_loss: 0.6298\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4313 - val_loss: 0.2458\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3135 - val_loss: 0.3219\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4866 - val_loss: 0.3600\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3015 - val_loss: 0.3018\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3925 - val_loss: 0.3993\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3512 - val_loss: 0.3617\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2734 - val_loss: 0.2803\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2927 - val_loss: 0.7141\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2624 - val_loss: 0.2160\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2164 - val_loss: 0.2131\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2134 - val_loss: 0.2153\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2144 - val_loss: 0.2180\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2156 - val_loss: 0.2142\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2211 - val_loss: 0.2190\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2147 - val_loss: 0.2296\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2201 - val_loss: 0.2709\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2217 - val_loss: 0.2258\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2071 - val_loss: 0.2102\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2051 - val_loss: 0.2110\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2045 - val_loss: 0.2120\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2044 - val_loss: 0.2125\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2038 - val_loss: 0.2126\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2053 - val_loss: 0.2106\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2041 - val_loss: 0.2192\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2036 - val_loss: 0.2191\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2021 - val_loss: 0.2097\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2009 - val_loss: 0.2103\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2014 - val_loss: 0.2099\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2005 - val_loss: 0.2097\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2009 - val_loss: 0.2099\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2002 - val_loss: 0.2093\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2007 - val_loss: 0.2103\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2005 - val_loss: 0.2130\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2006 - val_loss: 0.2123\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2006 - val_loss: 0.2100\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2006 - val_loss: 0.2107\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1997 - val_loss: 0.2110\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2003 - val_loss: 0.2103\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.280000142287463e-05.\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1989 - val_loss: 0.2100\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1989 - val_loss: 0.2099\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1990 - val_loss: 0.2100\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1990 - val_loss: 0.2101\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1987 - val_loss: 0.2098\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1993 - val_loss: 0.2102\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1994 - val_loss: 0.2100\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-06.\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.1990 - val_loss: 0.2100\n",
      "Fold 3 NN_ss: 0.20933\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 6s 14ms/step - loss: 33.8640 - val_loss: 2.3281\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.9844 - val_loss: 1.5336\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.5172 - val_loss: 1.6684\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.2842 - val_loss: 1.0959\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.1363 - val_loss: 1.1017\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.1451 - val_loss: 1.0083\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.0023 - val_loss: 0.9302\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.9525 - val_loss: 0.6726\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.7555 - val_loss: 1.1491\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8816 - val_loss: 0.9142\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.7789 - val_loss: 0.4770\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5315 - val_loss: 1.0346\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.8857 - val_loss: 0.7100\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 6s 17ms/step - loss: 0.8003 - val_loss: 0.5860\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 5s 15ms/step - loss: 0.9813 - val_loss: 0.9698\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.9225 - val_loss: 0.7529\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6661 - val_loss: 0.5619\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5878 - val_loss: 1.3806\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.6308 - val_loss: 0.5832\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5558 - val_loss: 0.5622\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.5561 - val_loss: 0.5638\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.5587 - val_loss: 0.5683\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5602 - val_loss: 0.7937\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5958 - val_loss: 0.5605\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5612 - val_loss: 0.5670\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.5558 - val_loss: 0.5666\n",
      "Fold 4 NN_ss: 0.47704\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 49.4667 - val_loss: 1.9102\n",
      "Epoch 2/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.4287 - val_loss: 0.3995\n",
      "Epoch 3/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.7925 - val_loss: 0.7235\n",
      "Epoch 4/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.7755 - val_loss: 1.3001\n",
      "Epoch 5/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 0.7254 - val_loss: 0.5903\n",
      "Epoch 6/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.5186 - val_loss: 0.5464\n",
      "Epoch 7/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 0.4870 - val_loss: 0.5036\n",
      "Epoch 8/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4318 - val_loss: 0.3532\n",
      "Epoch 9/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.3451 - val_loss: 0.5573\n",
      "Epoch 10/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4468 - val_loss: 0.2421\n",
      "Epoch 11/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.4257 - val_loss: 0.4672\n",
      "Epoch 12/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3866 - val_loss: 0.4089\n",
      "Epoch 13/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3291 - val_loss: 0.2823\n",
      "Epoch 14/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3291 - val_loss: 0.4094\n",
      "Epoch 15/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3404 - val_loss: 0.2419\n",
      "Epoch 16/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2973 - val_loss: 0.2307\n",
      "Epoch 17/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2379 - val_loss: 0.2879\n",
      "Epoch 18/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2405 - val_loss: 0.2212\n",
      "Epoch 19/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2913 - val_loss: 0.2218\n",
      "Epoch 20/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2446 - val_loss: 0.3885\n",
      "Epoch 21/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3260 - val_loss: 0.3658\n",
      "Epoch 22/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.3583 - val_loss: 0.2671\n",
      "Epoch 23/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2746 - val_loss: 0.3270\n",
      "Epoch 24/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2740 - val_loss: 0.2251\n",
      "Epoch 25/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2811 - val_loss: 0.3391\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 26/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2261 - val_loss: 0.2191\n",
      "Epoch 27/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2116 - val_loss: 0.2140\n",
      "Epoch 28/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2109 - val_loss: 0.2144\n",
      "Epoch 29/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2090 - val_loss: 0.2208\n",
      "Epoch 30/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2085 - val_loss: 0.2143\n",
      "Epoch 31/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2085 - val_loss: 0.2120\n",
      "Epoch 32/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2088 - val_loss: 0.2170\n",
      "Epoch 33/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2096 - val_loss: 0.2348\n",
      "Epoch 34/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2067 - val_loss: 0.2147\n",
      "Epoch 35/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2068 - val_loss: 0.2276\n",
      "Epoch 36/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.2082 - val_loss: 0.2297\n",
      "Epoch 37/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.2079 - val_loss: 0.2141\n",
      "Epoch 38/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 0.2078 - val_loss: 0.2142\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "Epoch 39/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.2002 - val_loss: 0.2139\n",
      "Epoch 40/1000\n",
      "336/336 [==============================] - 5s 13ms/step - loss: 0.1997 - val_loss: 0.2118\n",
      "Epoch 41/1000\n",
      "336/336 [==============================] - 5s 15ms/step - loss: 0.1995 - val_loss: 0.2110\n",
      "Epoch 42/1000\n",
      "336/336 [==============================] - 4s 13ms/step - loss: 0.1986 - val_loss: 0.2137\n",
      "Epoch 43/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1986 - val_loss: 0.2133\n",
      "Epoch 44/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1986 - val_loss: 0.2108\n",
      "Epoch 45/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1980 - val_loss: 0.2119\n",
      "Epoch 46/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1986 - val_loss: 0.2139\n",
      "Epoch 47/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1979 - val_loss: 0.2126\n",
      "Epoch 48/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1974 - val_loss: 0.2124\n",
      "Epoch 49/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1971 - val_loss: 0.2136\n",
      "Epoch 50/1000\n",
      "336/336 [==============================] - 5s 14ms/step - loss: 0.1964 - val_loss: 0.2117\n",
      "Epoch 51/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1957 - val_loss: 0.2126\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "Epoch 52/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1943 - val_loss: 0.2126\n",
      "Epoch 53/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1937 - val_loss: 0.2125\n",
      "Epoch 54/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1925 - val_loss: 0.2140\n",
      "Epoch 55/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1938 - val_loss: 0.2123\n",
      "Epoch 56/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1935 - val_loss: 0.2130\n",
      "Epoch 57/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1929 - val_loss: 0.2142\n",
      "Epoch 58/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1931 - val_loss: 0.2141\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.280000142287463e-05.\n",
      "Epoch 59/1000\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 0.1930 - val_loss: 0.2128\n",
      "Fold 5 NN_ss: 0.21083\n"
     ]
    }
   ],
   "source": [
    "model_name = 'NN_ss'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train1)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "\n",
    "try:\n",
    "    features_to_consider.remove('pred_ss')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train1[features_to_consider] = train1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "test1[features_to_consider] = test1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "\n",
    "train1[pred_name] = 0\n",
    "test1[target_name] = 0\n",
    "test_predictions_nn1 = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes, obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]], values[indexes[1]], values[indexes[2]], values[indexes[3]]]\n",
    "    \n",
    "    X_train = train1.loc[train1.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train1.loc[train1.time_id.isin(indexes), target_name]\n",
    "    X_test = train1.loc[train1.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train1.loc[train1.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.008),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = StandardScaler()         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=1024,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1, -1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true=y_test, y_pred=preds), 5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt = scaler.transform(test_nn[features_to_consider].values)\n",
    "    test_predictions_nn1 += model.predict([test1['stock_id'], tt]).reshape(1, -1)[0].clip(0, 1e10)/n_folds\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42727b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T13:48:59.356315Z",
     "iopub.status.busy": "2021-09-18T13:48:59.355531Z",
     "iopub.status.idle": "2021-09-18T13:48:59.382503Z",
     "shell.execute_reply": "2021-09-18T13:48:59.381914Z"
    },
    "papermill": {
     "duration": 11.951309,
     "end_time": "2021-09-18T13:48:59.382665",
     "exception": false,
     "start_time": "2021-09-18T13:48:47.431356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.001948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.001948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001853\n",
       "1   0-32  0.001948\n",
       "2   0-34  0.001948"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "pred_0 = test_predictions_nn * 0.6 + predictions_lgb0 * 0.4\n",
    "pred_1 = test_predictions_nn1 * 0.65 + predictions_lgb1 * 0.35\n",
    "test[target_name] = 0.4 * pred_0 + 0.6 * pred_1\n",
    "\n",
    "display(test[['row_id', target_name]].head())\n",
    "test[['row_id', target_name]].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bdedb",
   "metadata": {
    "papermill": {
     "duration": 11.829631,
     "end_time": "2021-09-18T13:49:23.179668",
     "exception": false,
     "start_time": "2021-09-18T13:49:11.350037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7264.180444,
   "end_time": "2021-09-18T13:49:37.508052",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-18T11:48:33.327608",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
